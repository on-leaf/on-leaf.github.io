---
layout: single
title:  "4.Dimensionality Reduction with Principal Component Analysis (2)"
category: "Mathematics for Machine Learning"
tag: [Machine Learning, Artificial Intelligence, Math]
header:
  teaser: #../images/2025-07-08-CT_6/image-20250708184503610.png
toc: true
toc_sticky: true
toc_label: CONTENTS
toc_icon: "fa-solid fa-seedling" # More icons: https://fontawesome.com/v6/search?ic=free
author_profile: false
sidebar:
    nav: "counts"
search: true # Change true to false if you don't want to have this article be searched 
redirect_from:
    - /Machine Learning/MfML_9
use_math: true
---

**[Reference]** <br>
$\bullet$ [MATHEMATICS FOR MACHINE LEARNING](https://mml-book.github.io/)
{: .notice--success}

# Introduction 
{% include start-box.html class="math-box"%}
Working directly with high-dimensional data, such as images, comes with some difficulties: hard to analyze/interpretation/visualization and storage of the data vectors can be expensive.
However, high-dimensional data is often overcomplete.
<div class="indented-paragraph" markdown="1">
Dimensionality of data can be reduced and it allows us to work with a more compact representation of the data, ideally without losing information.
</div>
In this chapter, we will discuss **_principal component analysis_** (PCA), an algorithm for linear dimensionality reduction.
{% include end-box.html %}