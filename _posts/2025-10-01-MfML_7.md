---
layout: single
title:  "3.Matrix Decomposition (2)"
category: "Mathematics for Machine Learning"
tag: [Machine Learning, Artificial Intelligence, Math]
header:
  teaser: #../images/2025-07-08-CT_6/image-20250708184503610.png
toc: true
toc_sticky: true
toc_label: CONTENTS
toc_icon: "fa-solid fa-seedling" # More icons: https://fontawesome.com/v6/search?ic=free
author_profile: false
sidebar:
    nav: "counts"
search: true # Change true to false if you don't want to have this article be searched 
redirect_from:
    - /Machine Learning/MfML_7
use_math: true
---

**[Reference]** <br>
$\bullet$ [MATHEMATICS FOR MACHINE LEARNING](https://mml-book.github.io/)
{: .notice--success}

# Introduction 
{% include start-box.html class="math-box"%}
In this chapter, three aspects of matrices will be presented:
<div class="indented-paragraph" markdown="1">
- How to summarize matrices
- How matrices can be decomposed
- How these decomposition can be used for matrix approximation
</div>
{% include end-box.html %}

# 4.Eigendecomposition and Diagonalization
{% include start-box.html class="math-box"%}
A diagonal matrix is a matrix that has value zero on all off-diagonal elements:

$$\mathbf{D} = \begin{bmatrix} 
c_1 & \dots & 0 \\ 
\vdots & \ddots & \vdots \\ 
0 & \dots & c_n 
\end{bmatrix} \tag{4.1}$$

A diagonal matrix allow fast computation of determinants, power, and inverses.
<div class="indented-paragraph" markdown="1">
- $$\det(\mathbf{D}) = \prod_{i=1}^{n} c_i = c_1 \cdot c_2 \cdots c_n$$
- $$\mathbf{D}^k = \begin{pmatrix} c_1^k & & 0 \\ & \ddots & \\ 0 & & c_n^k \end{pmatrix}$$
- $$\mathbf{D}^{-1} = \begin{pmatrix} c_1^{-1} & & 0 \\ & \ddots & \\ 0 & & c_n^{-1} \end{pmatrix}$$
</div>

We will discuss how to transform matrices into diagonal form.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 4.19 (Diagonalizable)**<br>
A matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ is **_diagonalizable_** if there exists an invertible matrix $\mathbf{P} \in \mathbb{R}^{n \times n}$ such that $\mathbf{D} = \mathbf{P}^{-1}\mathbf{A}\mathbf{P}$.

In the following, we will see that diagonalizing a matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ is a way of expressing the same linear mapping but in another basis - egenvectors of $\mathbf{A}$.

Let $$\begin{align} &\mathbf{A} \in \mathbb{R}^{n \times n} \\ &\lambda_1, \dots, \lambda_n: \text{a set of scalars} \\ &p_1, \dots, p_n: \text{a set of vectors in $\mathbb{R}^n$} \end{align}$$ and $$\begin{align} &P := [p_1, \dots, p_n] \\ &\mathbf{D} \in \mathbb{R}^{n \times n}: \text{a diagonal matrix with diagonal entries $\lambda_1, \dots, \lambda_n$}  \end{align}$$.

If $\lambda_1, \dots, \lambda_n$ are the eigenvalues of $\mathbf{A}$, and $p_1, \dots, p_n$ are corresponding eigenvectors of $\mathbf{A}$, we can show that 

$$\mathbf{A}\mathbf{P} = \mathbf{P}\mathbf{D} \tag{4.2}$$

Because, 

$$\begin{align}
\mathbf{A}\mathbf{P} &= \mathbf{A}[\vec{p}_1, \dots, \vec{p}_n] = [\mathbf{A}\vec{p}_1, \dots, \mathbf{A}\vec{p}_n], \tag{4.3} \\
\mathbf{P}\mathbf{D} &= [\vec{p}_1, \dots, \vec{p}_n] \begin{bmatrix} \lambda_1 & & 0 \\ & \ddots & \\ 0 & & \lambda_n \end{bmatrix} = [\lambda_1\vec{p}_1, \dots, \lambda_n\vec{p}_n]. \tag{4.4}
\end{align}$$

and it implies that 

$$\begin{align}
    \mathbf{A}\vec{p}_1 &= \lambda_1\vec{p}_1 \tag{4.5} \\
    &\vdots \nonumber \\
    \mathbf{A}\vec{p}_n &= \lambda_n\vec{p}_n. \tag{4.6}
\end{align}$$

Therefore, the columns of $\mathbf{P}$ must be eigenvectors of $\mathbf{A}$.

Our definition of diagonalization ($\mathbf{D} = \mathbf{P}^{-1}\mathbf{A}\mathbf{P}$) requires that $\mathbf{P} \in \mathbb{R}^{n \times n}$.
<div class="indented-paragraph" markdown="1">
$\mathbf{P}$ has full rank. $\rightarrow$ $\mathbf{A}$ has $n$ linearly independent eigenvectors $p_1, \dots, p_n$ and they form a basis of $\mathbb{R}^n$.
</div>

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
By using properties of diagonal matrices $\mathbf{D}$, we can easily compute various form of $\mathbf{A}$.

- $\mathbf{A}^k = (\mathbf{P}\mathbf{D}\mathbf{P}^{-1})^k = \mathbf{P}\mathbf{D}^k\mathbf{P}^{-1} \quad (4.7)$
- $\det(\mathbf{A}) = \det(\mathbf{P}\mathbf{D}\mathbf{P}^{-1}) = \det(\mathbf{P})\det(\mathbf{D})\det(\mathbf{P}^{-1}) = \det(\mathbf{D}) = \prod_i d_{ii} \quad (4.8)$
{% include end-box.html %}
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Theorem 4.20 (Eigendecomposition)**<br>
A square matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ can be factored into

$$\mathbf{A} = \mathbf{P}\mathbf{D}\mathbf{P}^{-1} \tag{4.9},$$

where the matrix $\mathbf{P}$ is the matrix whose columns are $n$ linearly independent eigenvectors of $\mathbf{A}$, and $\mathbf{D}$ is the diagonal matrix whose diagonal entries are the corresponding eigenvalues of $\mathbf{A}$.

<div class="indented-paragraph" markdown="1">
A matrix $\mathbf{A}$ is diagonalizable if and only if it has $n$ linearly independent eigenvectors.<br>
More specifically, only non-defective matrices can be diagonalized.
</div>
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Theorem 4.21**<br>
A _symmetric matrix_ $\mathbf{S} \in \mathbb{R}^{n \times n}$ can always be diagonalized.

This statemnet follows directly from the spectral theorem (4.15). 
Moreover, the spectral theorem states that we can find an ONB of eigenvectors of $\mathbb{R}^n$. 
This makes $\mathbf{P}$ an orthogonal matrix so that $\mathbf{D} = \mathbf{P}^\top \mathbf{A} \mathbf{P}$.
<div class="indented-paragraph" markdown="1">
A symmetric matrix is always orthogonally diagonalizable.
</div>
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Geometric Intuition for the Eigendecomposition**<br>
We can interpret the eigendecomposition of a matrix as follows:

{% include start-box.html class="math-box-inner" font_size="1.0em" padding_bottom="1.5em"%}
  {% include start-side-by-side.html
  image_src="../images/2025-10-01-MfML_7/Fig_1.png"
  image_alt="Fig.4.1. Geometric intuition for the eigendecomposition."
  image_width="45%"
  margin_top = "-4em"
  margin_bottom = "-4em"
  %}
  1.$\mathbf{P}^{-1}$: Changes the perspective from the standard coordinate system $(\vec{e}_1, \vec{e}_2)$ to the eigenvector coordinate system (eigenbasis, $\vec{p}_1, \vec{p}_2$).<br><br>
  2.$\mathbf{D}$: With respect to the new eigenvector axes, this step simply stretches or shrinks the space along each axis.<br><br>
  3.$\mathbf{P}$: Takes the scaled result from the eigenbasis and transforms it back to the original standard coordinate system.
  {% include end-side-by-side.html %}
{% include end-box.html %}
The seemingly complex transformation $\mathbf{A}$ (top-left $\rightarrow$ top-right) is equivalent to a three-step process: changing to a special coordinate system defined by the eigenvectors ($\mathbf{P}^{âˆ’1}$), performing a simple scaling along those new axes ($\mathbf{D}$), and then changing back to the original coordinate system ($\mathbf{P}$).
{% include end-box.html %}
{% include end-box.html %}

# 5.Singular Value Decomposition
{% include start-box.html class="math-box"%}

{% include end-box.html %}






<div class="indented-paragraph" markdown="1">

</div>

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition . ()**<br>
{% include end-box.html %}


{% include start-box.html class="math-box"%}

{% include end-box.html %}