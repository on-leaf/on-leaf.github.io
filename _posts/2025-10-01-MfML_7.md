---
layout: single
title:  "3.Matrix Decomposition (2)"
category: "Mathematics for Machine Learning"
tag: [Machine Learning, Artificial Intelligence, Math]
header:
  teaser: #../images/2025-07-08-CT_6/image-20250708184503610.png
toc: true
toc_sticky: true
toc_label: CONTENTS
toc_icon: "fa-solid fa-seedling" # More icons: https://fontawesome.com/v6/search?ic=free
author_profile: false
sidebar:
    nav: "counts"
search: true # Change true to false if you don't want to have this article be searched 
redirect_from:
    - /Machine Learning/MfML_7
use_math: true
---

**[Reference]** <br>
$\bullet$ [MATHEMATICS FOR MACHINE LEARNING](https://mml-book.github.io/)
{: .notice--success}

# Introduction 
{% include start-box.html class="math-box"%}
In this chapter, three aspects of matrices will be presented:
<div class="indented-paragraph" markdown="1">
- How to summarize matrices
- How matrices can be decomposed
- How these decomposition can be used for matrix approximation
</div>
{% include end-box.html %}

# 4.Eigendecomposition and Diagonalization
{% include start-box.html class="math-box"%}
A diagonal matrix is a matrix that has value zero on all off-diagonal elements:

$$\mathbf{D} = \begin{bmatrix} 
c_1 & \dots & 0 \\ 
\vdots & \ddots & \vdots \\ 
0 & \dots & c_n 
\end{bmatrix} \tag{4.1}$$

A diagonal matrix allow fast computation of determinants, power, and inverses.
<div class="indented-paragraph" markdown="1">
- $$\det(\mathbf{D}) = \prod_{i=1}^{n} c_i = c_1 \cdot c_2 \cdots c_n$$
- $$\mathbf{D}^k = \begin{pmatrix} c_1^k & & 0 \\ & \ddots & \\ 0 & & c_n^k \end{pmatrix}$$
- $$\mathbf{D}^{-1} = \begin{pmatrix} c_1^{-1} & & 0 \\ & \ddots & \\ 0 & & c_n^{-1} \end{pmatrix}$$
</div>

We will discuss how to transform matrices into diagonal form.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 4.19 (Diagonalizable)**<br>
A matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ is **_diagonalizable_** if there exists an invertible matrix $\mathbf{P} \in \mathbb{R}^{n \times n}$ such that $\mathbf{D} = \mathbf{P}^{-1}\mathbf{A}\mathbf{P}$.

In the following, we will see that diagonalizing a matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ is a way of expressing the same linear mapping but in another basis - egenvectors of $\mathbf{A}$.

Let $$\begin{align} &\mathbf{A} \in \mathbb{R}^{n \times n} \\ &\lambda_1, \dots, \lambda_n: \text{a set of scalars} \\ &p_1, \dots, p_n: \text{a set of vectors in $\mathbb{R}^n$} \end{align}$$ and $$\begin{align} &P := [p_1, \dots, p_n] \\ &\mathbf{D} \in \mathbb{R}^{n \times n}: \text{a diagonal matrix with diagonal entries $\lambda_1, \dots, \lambda_n$}  \end{align}$$.

If $\lambda_1, \dots, \lambda_n$ are the eigenvalues of $\mathbf{A}$, and $p_1, \dots, p_n$ are corresponding eigenvectors of $\mathbf{A}$, we can show that 

$$\mathbf{A}\mathbf{P} = \mathbf{P}\mathbf{D} \tag{4.2}$$

Because, 

$$\begin{align}
\mathbf{A}\mathbf{P} &= \mathbf{A}[\vec{p}_1, \dots, \vec{p}_n] = [\mathbf{A}\vec{p}_1, \dots, \mathbf{A}\vec{p}_n], \tag{4.3} \\
\mathbf{P}\mathbf{D} &= [\vec{p}_1, \dots, \vec{p}_n] \begin{bmatrix} \lambda_1 & & 0 \\ & \ddots & \\ 0 & & \lambda_n \end{bmatrix} = [\lambda_1\vec{p}_1, \dots, \lambda_n\vec{p}_n]. \tag{4.4}
\end{align}$$

and it implies that 

$$\begin{align}
    \mathbf{A}\vec{p}_1 &= \lambda_1\vec{p}_1 \tag{4.5} \\
    &\vdots \nonumber \\
    \mathbf{A}\vec{p}_n &= \lambda_n\vec{p}_n. \tag{4.6}
\end{align}$$

Therefore, the columns of $\mathbf{P}$ must be eigenvectors of $\mathbf{A}$.

Our definition of diagonalization ($\mathbf{D} = \mathbf{P}^{-1}\mathbf{A}\mathbf{P}$) requires that $\mathbf{P} \in \mathbb{R}^{n \times n}$.
<div class="indented-paragraph" markdown="1">
$\mathbf{P}$ has full rank. $\rightarrow$ $\mathbf{A}$ has $n$ linearly independent eigenvectors $p_1, \dots, p_n$ and they form a basis of $\mathbb{R}^n$.
</div>

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
By using properties of diagonal matrices $\mathbf{D}$, we can easily compute various form of $\mathbf{A}$.

- $\mathbf{A}^k = (\mathbf{P}\mathbf{D}\mathbf{P}^{-1})^k = \mathbf{P}\mathbf{D}^k\mathbf{P}^{-1} \quad (4.7)$
- $\det(\mathbf{A}) = \det(\mathbf{P}\mathbf{D}\mathbf{P}^{-1}) = \det(\mathbf{P})\det(\mathbf{D})\det(\mathbf{P}^{-1}) = \det(\mathbf{D}) = \prod_i d_{ii} \quad (4.8)$
{% include end-box.html %}
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Theorem 4.20 (Eigendecomposition)**<br>
A square matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ can be factored into

$$\mathbf{A} = \mathbf{P}\mathbf{D}\mathbf{P}^{-1} \tag{4.9},$$

where the matrix $\mathbf{P}$ is the matrix whose columns are $n$ linearly independent eigenvectors of $\mathbf{A}$, and $\mathbf{D}$ is the diagonal matrix whose diagonal entries are the corresponding eigenvalues of $\mathbf{A}$.

<div class="indented-paragraph" markdown="1">
A matrix $\mathbf{A}$ is diagonalizable if and only if it has $n$ linearly independent eigenvectors.<br>
More specifically, only non-defective matrices can be diagonalized.
</div>
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Theorem 4.21**<br>
A _symmetric matrix_ $\mathbf{S} \in \mathbb{R}^{n \times n}$ can always be diagonalized.

This statemnet follows directly from the spectral theorem (4.15). 
Moreover, the spectral theorem states that we can find an ONB of eigenvectors of $\mathbb{R}^n$. 
This makes $\mathbf{P}$ an orthogonal matrix so that $\mathbf{D} = \mathbf{P}^\top \mathbf{A} \mathbf{P}$.
<div class="indented-paragraph" markdown="1">
A symmetric matrix is always orthogonally diagonalizable.
</div>
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Geometric Intuition for the Eigendecomposition**<br>
We can interpret the eigendecomposition of a matrix as follows:

{% include start-box.html class="math-box-inner" font_size="1.0em" padding_bottom="1.5em"%}
  {% include start-side-by-side.html
  image_src="../images/2025-10-01-MfML_7/Fig_1.png"
  image_alt="Fig.4.1. Geometric intuition for the eigendecomposition."
  image_width="45%"
  margin_top = "-4em"
  margin_bottom = "-4em"
  %}
  1.$\mathbf{P}^{-1}$: Changes the perspective from the standard coordinate system $(\vec{e}_1, \vec{e}_2)$ to the eigenvector coordinate system (eigenbasis, $\vec{p}_1, \vec{p}_2$).<br><br>
  2.$\mathbf{D}$: With respect to the new eigenvector axes, this step simply stretches or shrinks the space along each axis.<br><br>
  3.$\mathbf{P}$: Takes the scaled result from the eigenbasis and transforms it back to the original standard coordinate system.
  {% include end-side-by-side.html %}
{% include end-box.html %}
The seemingly complex transformation $\mathbf{A}$ (top-left $\rightarrow$ top-right) is equivalent to a three-step process: changing to a special coordinate system defined by the eigenvectors ($\mathbf{P}^{âˆ’1}$), performing a simple scaling along those new axes ($\mathbf{D}$), and then changing back to the original coordinate system ($\mathbf{P}$).
{% include end-box.html %}
{% include end-box.html %}

# 5.Singular Value Decomposition
{% include start-box.html class="math-box"%}
The singular value decomposition (SVD) of a matrix is a central matrix decomposition method in linear algebra.
SVD can be applied to all matrices, not only to square matrices, and it always exists.
In the following, we will see that the SVD of a matrix $\mathbf{A}$, which represents a linear mapping $\Phi: V \rightarrow W$, quantifies the change between the underlying geometry of these two vector spaces.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Theorem 4.22 (SVD Theorem)**<br>
Let $\mathbf{A} \in \mathbb{R}^{m \times n}$ be a **_rectangular matrix_** of rank $r \in [0,\min(m,n)]$.
The SVD of $\mathbf{A}$ is a decomposition of the form  with an orthogonal matrix $\mathbf{U} \in \mathbb{R}^{m \times m}$ with column vectors $$\vec{u}_{i}$$, $i=1,\dots,m$, and an orthogonal matrix $\mathbf{V} \in \mathbb{R}^{n \times n}$ with column vectors $$\vec{v}_{j}$$, $j=1,\dots, n$ and $m \times n$ matrix $\Sigma$ with $$\Sigma_{ii}=\sigma_{i} \ge 0$$ and $$\Sigma_{ij}=0$$, $i \neq j$.

<figure style="display: flex; flex-direction: column; align-items: center; margin-top: 0.5em; margin-bottom: 0.5em;">
  <img src="../images/2025-10-01-MfML_7/Fig_2.png" alt="" 
       style="width: 40%; height: auto;">
   <figcaption style="font-size: 20px; margin-top: -0.5em;">
   (5.1)
   </figcaption>
</figure> 

The diagonal entries $$\sigma_{i}$$, $i=1,\dots,r$, of $\Sigma$ are called the **_singular values_**, $$\vec{u}_{i}$$ are called the **_left-singular vectors_**, and $$\vec{v}_{i}$$ are called **_right-singular vectors_**.
By convention, the singular values are oreder, $\sigma_{1} \ge \sigma_{2} \ge \dots \ge \sigma_{r} \ge 0$.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
The _singular value matrix_ $\mathbf{\Sigma}$ is unique, but it requires some attention. 
Observe that the $\mathbf{\Sigma} \in \mathbb{R}^{m \times n}$ is rectangular. 
<div class="indented-paragraph" markdown="1">
$m \times n$ size same as $\mathbf{A}$
</div>
This means that $\mathbf{\Sigma}$ has a diagonal submatrix that contains the singular values and needs additional zero padding. 

Specifically, if $m > n$, then the matrix $\mathbf{\Sigma}$ has diagonal structure up to row $n$ and then consists of $\mathbf{0}^\top$ row vectors from $n+1$ to $m$ below so that

$$\mathbf{\Sigma} = \begin{bmatrix} \sigma_1 & 0 & 0 \\ 0 & \ddots & 0 \\ 0 & 0 & \sigma_n \\ \hline 0 & \dots & 0 \\ \vdots & & \vdots \\ 0 & \dots & 0 \end{bmatrix}. \tag{5.2}$$

If $m < n$, the matrix $\mathbf{\Sigma}$ has a diagonal structure up to column $m$ and columns that consist of $\mathbf{0}$ from $m+1$ to $n$:

$$\mathbf{\Sigma} = \left[ \begin{array}{ccc|ccc}\sigma_1 & 0 & 0 & 0 & \dots & 0 \\0 & \ddots & 0 & \vdots & \ddots & \vdots \\0 & 0 & \sigma_m & 0 & \dots & 0\end{array} \right]. \tag{5.3}$$

{% include end-box.html %}

## 5-1) Geometric Intuitions for the SVD


{% include end-box.html %}






{% include end-box.html %}






<div class="indented-paragraph" markdown="1">

</div>

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition . ()**<br>
{% include end-box.html %}


{% include start-box.html class="math-box"%}

{% include end-box.html %}









