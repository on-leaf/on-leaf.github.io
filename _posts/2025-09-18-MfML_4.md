---
layout: single
title:  "2.Analytic Geometry (1)"
category: "Mathematics for Machine Learning"
tag: [Machine Learning, Artificial Intelligence, Math]
header:
  teaser: #../images/2025-07-08-CT_6/image-20250708184503610.png
toc: true
toc_sticky: true
toc_label: CONTENTS
toc_icon: "fa-solid fa-seedling" # More icons: https://fontawesome.com/v6/search?ic=free
author_profile: false
sidebar:
    nav: "counts"
search: true # Change true to false if you don't want to have this article be searched 
redirect_from:
    - /Machine Learning/MfML_4
use_math: true
---

**[Reference]** <br>
$\bullet$ [MATHEMATICS FOR MACHINE LEARNING](https://mml-book.github.io/)
{: .notice--success}

# Introduction 
{% include start-box.html class="math-box"%}
We will add some geometric interpretation and intuition to concenpts of vectors, vector spaces, and linear mappings. 
To look at geometric vectors and compute their lengths and distances or angles between two vectors, we need to equip the vector space with an inner product that induces the geometry of the vector space. 

- Inner product and their corresponding norms and metrics
    <div class="indented-paragraph" markdown="1">
    Similarity and distances for support vector machine in Ch.12
    </div>
- Lengths and angles between vectors
    <div class="indented-paragraph" markdown="1">
    Orthogonal projections for principal component analysis in Ch.10<br>
    Regression via maximum likelihood estimation in Ch.9
    </div>
{% include end-box.html %}

# 1. Norms
{% include start-box.html class="math-box"%}
We will discuss the notion of the length of vectors using the concept of a norm.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 3.1 (Norm)**<br>
A **_norm_** on a vector space $V$ is a function.
<div class="indented-paragraph" markdown="1">
Assigns each vector $\vec{x}$ its **_length_** $\Vert \vec{x} \Vert \in \mathbb{R}$
</div>

$$\Vert \cdot \Vert: V \rightarrow \mathbb{R} \tag{1.1}$$

$$x \mapsto \Vert x \Vert \tag{1.2}$$

For all $\lambda \in \mathbb{R}$ and $\vec{x}, \vec{y} \in V$ the following hold:

- Absolutely homogeneous: $\Vert \lambda \vec{x} \Vert = \vert \lambda \vert \Vert \vec{x} \Vert$
- Triangle inequality: $\Vert \vec{x} + \vec{y} \Vert \le \Vert x \Vert + \Vert \vec{y} \Vert$
- Positive definite: $\Vert \vec{x} \Vert \ge 0$ and $\Vert \vec{x} \Vert = 0 \Leftrightarrow \vec{x} =\vec{0}$
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Example (Manhattan Norm)**<br>
The **_Manhattan norm_**, which is also called $\ell_1$ norm, on $\mathbb{R}^n$ is defined for $\vec{x} \in \mathbb{R}^n$ as

$$\Vert \vec{x} \Vert_1 := \sum_{i=1}^{n} \vert x_i \vert. \tag{1.3}$$

{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Example (Euclidean Norm)**<br>
The **_Euclidean norm_**, which is also called $\ell_2$ norm, of $\vec{x} \in \mathbb{R}^n$ is defined as

$$\Vert \vec{x} \Vert_2 := \sqrt{\sum_{i=1}^{n} x_i^2} = \sqrt{\vec{x}^{\top} \vec{x}}. \tag{1.4}$$

The Euclidean norm will be used by default if not stated otherwise.
{% include end-box.html %}
{% include end-box.html %}

# 2. Inner Products
{% include start-box.html class="math-box"%}
Inner products is used for the length of a vector and the angle or distance between two vectors.
A major purpose of inner products is to determine whether vectors are orthogonal to each other.

## 2-1) Dot Product
The **_scalar product/dot product_** in $\mathbb{R}^n$, which is given by 

$$\vec{x}^{\top} \vec{y} = \sum_{i=1}^{n} x_i y_i \tag{2.1}$$

This particular inner product is usually called dot product.
However, inner products are more general concepts with specific properties, which are introduced next section.

## 2-2) General Inner Products
A bilinear mapping $\Omega$ is a mapping with two arguments, and it is linear in each argument.

For a vector space $V$, it holds that for all $x,y,z \in V, \lambda \psi \in \mathbb{R}$ that

<center>$$\Omega(\lambda \vec{x} + \psi \vec{y}, z) = \lambda \Omega(\vec{x}, \vec{z}) + \psi \Omega(\vec{y}, \vec{z}) \tag{2.2}$$</center>
<center>$$\Omega(\vec{x}, \lambda \vec{y} + \psi z) = \lambda \Omega(\vec{x}, \vec{y}) + \psi \Omega(\vec{x}, \vec{z}) \tag{2.3}$$</center>

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 3.2**<br>
Let $V$ be a vector space and $\Omega: V \times V \rightarrow \mathbb{R}$ be bilinear mapping that takes two vectors and maps them onto a real number.
Then, 

- **_Symmetric_**
    <div class="indented-paragraph" markdown="1">
    $$\Omega(\vec{x}, \vec{y}) = \Omega(\vec{y}, \vec{x})$$ for all $\vec{x}, \vec{y} \in V$
    </div>

- **_Positive definite_**
    <div class="indented-paragraph" markdown="1">
    $$\forall \vec{x} \in V \text{\\} \{ \vec{0} \}: \Omega(\vec{x}, \vec{x}) \gt 0, \Omega(\vec{0}, \vec{0}) = 0 \quad \text{(3.8)}$$
    </div>
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 3.3**<br>
Let $V$ be a vector space and $\Omega: V \times V \rightarrow \mathbb{R}$ be a bilinear mapping that takes two vectors and maps them onto a real number. 
Then,

- A positive definite, symmetric bilinear mapping $\Omega: V \times V \rightarrow \mathbb{R}$ is called an **_inner product_** on $V$.
    <div class="indented-paragraph" markdown="1">
    Typically write $<\vec{x}, \vec{y}>$
    </div>
- The pair $(V, <\cdot, \cdot>)$ is called an **_inner product space_** or (real) **_vector space with inner product_**. 
    <div class="indented-paragraph" markdown="1">
    If we use the dot product in (3.5), we call $(V, <\cdot, \cdot>)$ a **_Euclidean vector space_**
    </div>
{% include end-box.html %}

## 2-3) Symmetric, Positive Definite Matrices
Symmetric, positive definite matrices are defined via the inner product.
Consider an $n$-dimensional vector space $V$ with an inner product $<\cdot, \cdot>: V \times V \rightarrow \mathbb{R}$ and an ordered basis $B = (\vec{b}_1, \dots, \vec{b}_n)$ of $V$.
<div class="indented-paragraph" markdown="1">
Any vectors $\vec{x}, \vec{y} \in V$ can be written as linear combination of the basis vectors.<br>$$\vec{x} = \sum_{i=1}^{n} \psi_i \vec{b}_i \in V$$ and $$\vec{y} = \sum_{j=1}^{n} \lambda_j \vec{b}_j \in V$$ for $$\psi_i, \lambda_j \in \mathbb{R}$$
</div>

Due to the bilinearity of the inner product, it holds for all $\vec{x}, \vec{y} \in V$ that

$${\vec{x}, \vec{y}} = \left<\sum_{i=1}^{n}\psi_i \vec{b}_i, \sum_{j=1}^{n}\psi_j \vec{b}_j \right> = \sum_{i=1}^{n}\sum_{j=1}^{n}\psi_i <\vec{b}_i, \vec{b}_j>\lambda_j = \hat{\vec{x}}^{\top}A \hat{\vec{y}} \tag{2.4}$$

where $A_{ij} := <\vec{b}_i, \vec{b}_j>$ and $\hat{\vec{x}}, \hat{\vec{y}}$ are the coordinates of $\vec{x}$ and $\vec{y}$ with respect to the basis $B$.

<div class="indented-paragraph" markdown="1">
The inner product $<\cdot, \cdot>$ is uniquely determined through $A$<br>The symmetry of the inner product also means that $A$ is symmetry
</div>

The positive definiteness of the inner product implies that 

$$\forall \vec{x} \in V \text{\\} \{ \vec{0} \}: \vec{x}^{\top}A\vec{x} \gt 0 \tag{2.5}$$

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 3.4 (Symmetric, Positive Definite Matrix)**<br>
A symmetric matrix $A \in \mathbb{R}^{n \times n}$ that satisfies $$\forall \vec{x} \in V \text{\\} \{ \vec{0} \}: \vec{x}^{\top}A\vec{x} \gt 0$$ is called **_symmetric, positive definite_**, or just **_positive definte_**.

If only $\ge$ holds, $$\forall \vec{x} \in V \text{\\} \{ \vec{0} \}: \vec{x}^{\top}A\vec{x} \ge 0$$, then $A$ is called **_symmetric, positive semidefinite_**.

{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Theorem 3.5**<br>
For a _real-valued_, _finite-demensional vector space_ $V$ and an ordered basis $B$ of $V$, $<\cdot, \cdot>: V \times V \rightarrow \mathbb{R}$ is an inner product if and only if there exists a symmetric, positive definite matrix $A \in \mathbb{R}^{n \times n}$ with

$$<\vec{x}, \vec{y}> = \hat{\vec{x}}^{\top} A \hat{\vec{y}} \tag{2.6}$$

If $A \in \mathbb{R}^{n \times n}$ is symmetric and positive definite:
- The null space of $A$ consists of $\vec{0}$
    <div class="indented-paragraph" markdown="1">
    $A\vec{x} \neq \vec{0}$ if $\vec{x} \neq 0$
    </div>
- The diagonal elements $a_{ii}$ of $A$ are positive
{% include end-box.html %}
{% include end-box.html %}

# 3. Lengths and Distance 
{% include start-box.html class="math-box"%}
Inner products and norms are closely related.
<div class="indented-paragraph" markdown="1">
Any inner product induces a norm
</div>

$$\Vert \vec{x} \Vert := \sqrt{<\vec{x}, \vec{x}>} \tag{3.1}$$

However, not every norm is induced by an inner product.
We will focus on norms that are induced by inner products and introduce geometric concepts - lenghts, distances and angles.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Cauchy-Schwarz Inequality**<br>
For an inner product vector space $(V, <\cdot, \cdot>)$ the induced norm $\Vert \cdot \Vert$ satifies the Cauchy-Schwarz inequality

$$\vert <\vec{x}, \vec{y}>\vert \le \Vert x \Vert \Vert y \Vert \tag{3.2}$$

{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 3.6 (Distance and Metric)**<br>
Consider an inner product space $(V, <\cdot, \cdot>)$. 
Then the **_distance_** between $\vec{x}$ and $\vec{y}$ for $\vec{x}, \vec{y} \in V$ is defined as:

$$d(\vec{x}, \vec{y}) := \Vert \vec{x} - \vec{y} \Vert = \sqrt{<\vec{x} - \vec{y}, \vec{x} - \vec{y}>} \tag{3.3}$$

<div class="indented-paragraph" markdown="1">
If we use the dot product as the inner product, the distance is called **_Euclidean distance_**.<br>
The distance between vectors does not require an inner product.<br>
It we have a norm induced by an inner product, the distance may vary depending on the choice of the inner product.<br>
</div>

The mapping defined as below is called **_metric_**.

$$d: V \times V \rightarrow \mathbb{R} \tag{3.4}$$

$$(\vec{x}, \vec{y}) \mapsto d(\vec{x}, \vec{y}) \tag{3.5}$$

<div class="indented-paragraph" markdown="1">
A metric is a function $d$ that takes two elements, $\vec{x}$ and $\vec{y}$, from a space $V$ and returns a single real value.
</div>

A metric $d$ satisfies the following:
- $d$ is positive definite
    <div class="indented-paragraph" markdown="1">
    $d(\vec{x}, \vec{y}) \ge 0$ for all $\vec{x}, \vec{y} \in V$ and $d(x,y)=0 \Leftrightarrow \vec{x} = \vec{y}$
    </div>
- $d$ is symmetric
    <div class="indented-paragraph" markdown="1">
    $d(\vec{x}, \vec{y}) = d(\vec{y}, \vec{x})$ for all $vec{x}, \vec{y} \in V$
    </div>
- Triangle inequality 
    <div class="indented-paragraph" markdown="1">
    $d(\vec{x}, \vec{z}) \le d(\vec{x}, \vec{y}) + d(\vec{y}, \vec{z})$ for all $vec{x}, \vec{y}, \vec{z} \in V$ 
    </div>
{% include end-box.html %}
{% include end-box.html %}



# 4. Angles and Orthogonality
{% include start-box.html class="math-box"%}
Inner products are used to defining the anlge $\omega$ between two vectors.
We use the Cauchy-Schwarz inequality to define angles $\omega$ in inner product spcaes between two vectors $\vec{x}, \vec{y}$.

Assume that $\vec{x} \neq 0, \vec{y} \neq 0$. Then, 

$$-1 \le \frac{<\vec{x}, \vec{y}>}{\Vert \vec{x} \Vert \Vert \vec{y} \Vert} \le 1 \tag{4.1}$$

Therefore, there exists a unique $\omega \in [0, \pi]$, with 

$$\cos \omega = \frac{<\vec{x}, \vec{y}>}{\Vert \vec{x} \Vert \Vert \vec{y} \Vert}. \tag{4.2}$$

The number $\omega$ is the **_angle_** between the vectors.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 3.7 (Orthogonality)**<br>
Two vectors $\vec{x}$ and $\vec{y}$ are **_orthogonal_** if and only if $<\vec{x}, \vec{y}> = 0$.
<div class="indented-paragraph" markdown="1">
$\vec{x} \perp \vec{y}$<br>
Note that the $\vec{0}$ is orthogonal to every vector in the vector space.<br>
Orthogonality is defined by a given inner product.
</div>

Additionally, $\Vert \vec{x} \Vert = 1 = \Vert \vec{y} \Vert$, then $\vec{x}$ and $\vec{y}$ are **_orthonormal_**.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 3.8 (Orthogonal Matrix)**<br>
A square matrix $A \in \mathbb{R}^{n \times n}$ is an **_orthogonal matrix_** if and only if its columns are orthonormal.

$$AA^{\top} = I = A^{\top}A, \tag{4.3}$$

which implies that

$$A^{-1} = A^{\top}, \tag{4.4}$$

i.e., the inverse is obtained by transposing the matrix.
{% include end-box.html %}

Transformations by orthogonal matrices are special because the length of a vector and the angle between two vectors are unchanged.
- Unchanged length

$$\Vert A\vec{x}\Vert^2 = (A\vec{x})^\top(A\vec{x}) = \vec{x}^\top A^\top A\vec{x} = \vec{x}^\top I\vec{x} = \vec{x}^\top \vec{x} = \Vert\vec{x}\Vert^2 \tag{4.5}$$

- Unchanged angle

$$\cos \omega = \frac{(A\vec{x})^\top (A\vec{y})}{\Vert A\vec{x} \Vert \Vert A\vec{y} \Vert} = \frac{\vec{x}^\top A^\top A\vec{y}}{\sqrt{\vec{x}^\top A^\top A\vec{x} \vec{y}^\top A^\top A\vec{y}}} = \frac{\vec{x}^\top \vec{y}}{\Vert \vec{x} \Vert \Vert \vec{y} \Vert} \tag{4.6}$$

It  means that orthogonal matrices $A$ with $A^{\top}=A^{-1}$ preserve both angles and distances.
{% include end-box.html %}