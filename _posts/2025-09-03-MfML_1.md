---
layout: single
title:  "1.Linear Algebra (1)"
category: "Mathematics for Machine Learning"
tag: [Machine Learning, Artificial Intelligence, Math]
header:
  teaser: #../images/2025-07-08-CT_6/image-20250708184503610.png
toc: true
toc_sticky: true
toc_label: CONTENTS
toc_icon: "fa-solid fa-seedling" # More icons: https://fontawesome.com/v6/search?ic=free
author_profile: false
sidebar:
    nav: "counts"
search: true # Change true to false if you don't want to have this article be searched 
redirect_from:
    - /Machine Learning/MfML_1
use_math: true
---

**[Reference]** <br>
$\bullet$ [MATHEMATICS FOR MACHINE LEARNING](https://mml-book.github.io/)
{: .notice--success}

# Introduction 
{% include start-box.html class="math-box"%}
Linear algebra is the study of vectors and certain rules to manipulate vectors.
Here, a bold letter represents a vector. 
{% include end-box.html %}

# 1. Systems of Linear Equations
{% include start-box.html class="math-box"%}
<center>$$\begin{align*}a_{11}x_{1} + \cdots + a_{1n}x_{n} &= b_{1} \\&\vdots \\a_{m1}x_{1} + \cdots + a_{mn}x_{n} &= b_{m}\end{align*}, \text{where $a_{ij} \in \mathbb{R}$ and $b_i \in \mathbb{R}$.} \tag{1.1}$$</center>

Equation (1.1) is the general form of a **_system of linear equations_**, and $x_1, \dots, x_n$ are the **_unkowns_** of this system.
Every n-tuple $(x_1, \dots, x_n) \in \mathbb{R}^n$ that satisfies (1.1) is a **_solution_** of the linear equation system.

$$\begin{array}{c|c|c}\begin{aligned}x_1 + x_2 + x_3 &= 3 \\x_1 - x_2 + 2x_3 &= 2 \quad (1.2)\\2x_1 \ \ \ \ \ \ \ + 3x_3 &= 1 \end{aligned}&\begin{aligned}x_1 + x_2 + x_3 &= 3 \\x_1 - x_2 + 2x_3 &= 2 \quad (1.3)\\\ \ \ \ \ \ \ \ \ x_2 + 3x_3 &= 1 \end{aligned}&\begin{aligned}x_1 + x_2 + x_3 &= 3 \\x_1 - x_2 + 2x_3 &= 2 \quad (1.4)\\2x_1 \ \ \ \ \ \ \ + 3x_3 &= 5\end{aligned}\end{array}$$

Various system of linear equations is here. 
In general, system of linear equations are can be seperated into three types.
<div class="indented-paragraph" markdown="1">
No solution (equ 1.2) | Unique solution (equ 1.3) | Infinitely many solutions (equ 1.4)
</div>

For a systematic approach to solving systems of linear equations, we can use a useful compact notation.
<div class="indented-paragraph" markdown="1">
Collect the coefficients $a_{ij}$ into vectors and collect the vectors into matrices
</div>

For example, (1.1) can be written as

$$\begin{bmatrix}a_{11} \\\vdots \\a_{m1}\end{bmatrix} x_1 +\begin{bmatrix}a_{12} \\\vdots \\a_{m2}\end{bmatrix} x_2 +\cdots +\begin{bmatrix}a_{1n} \\\vdots \\a_{mn}\end{bmatrix} x_n =\begin{bmatrix}b_1 \\\vdots \\b_m\end{bmatrix} \quad (1.5) \Leftrightarrow \begin{bmatrix}a_{11} & \cdots & a_{1n} \\\vdots & \ddots & \vdots \\a_{m1} & \cdots & a_{mn}\end{bmatrix}\begin{bmatrix}x_1 \\\vdots \\x_n\end{bmatrix}=\begin{bmatrix}b_1 \\\vdots \\b_m\end{bmatrix} \quad (1.6)$$
{% include end-box.html %}


# 2. Matrices
{% include start-box.html class="math-box"%}
Matrices can be used to compactly represent systems of linear equations, but they also represent linear fucntions (linear mapping, see it later).
First, let's define what a matrix is and what kind of operations we can do with matrices. 

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 2.1 (Matrix)**<br>
With $m,n \in \mathbb{N}$ a real-valued $(m,n)$ matrix **_A_** is an $m\cdot n$-tuple of elements $a_{ij}, i=1,\dots, m, j=1,\dots, n$, which has $m$ rows and $n$ columns:

$$A =\begin{bmatrix}a_{11} & a_{12} & \cdots & a_{1n} \\a_{21} & a_{22} & \cdots & a_{2n} \\\vdots & \vdots & \ddots & \vdots \\a_{m1} & a_{m2} & \cdots & a_{mn}\end{bmatrix},
\quad a_{ij} \in \mathbb{R} \tag{2.1}$$

$(1,n)$-matrices are called **_rows / row vectors_** and $(m,1)$-matrices are called **_columns / column vectors_**.
$\mathbb{R}^{m\times n}$ is the set of all real-valued $(m,n)$-matrices. 
**_A_**$\in \mathbb{R}^{m \times n}$ can be equivalently represented as $\textbf{a} \in \mathbb{R}^{mn}$ by stacking all $n$ columns of the matrix into a long vector.
{% include end-box.html %}

## 2-1) Matrix Addition and Multiplication
The sum of two matrices **_A_** $\in \mathbb{R}^{m \times n}$, **_B_** $\in \mathbb{R}^{m \times n}$ is defined as the element-wise sum.

$$A + B := \begin{bmatrix}a_{11} + b_{11} & \cdots & a_{1n} + b_{1n} \\\vdots & \ddots & \vdots \\a_{m1} + b_{m1} & \cdots & a_{mn} + b_{mn}\end{bmatrix}\in \mathbb{R}^{m \times n} \tag{2.2}$$

For matrices **_A_** $\in \mathbb{R}^{m \times n}$, **_B_** $\in \mathbb{R}^{n \times k}$, the elements $c_{ij}$ of the product **_C=AB_** $\in \mathbb{R}^{m\times k}$ are computed as 

$$c_{ij} = \sum_{l=1}^{n} a_{il} b_{lj}, \quad i = 1, \ldots, m, \quad j = 1, \ldots, k. \tag{2.3}$$

Matrix multiplication is not commutative, i.e., **_AB_** $\neq$ **_BA_**.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 2.2 (Identity Matrix)**<br>
In $\mathbb{R}^{n \times n}$, we define the **_identity matrix_**

$$I_n := \begin{bmatrix} 1 & 0 & \cdots & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 0 & \cdots & 1 \end{bmatrix} \in \mathbb{R}^{n \times n} \tag{2.4}$$

as the $n\times n$-matrix containign 1 on the diagonal and 0 everywhre else.
{% include end-box.html %}

Then, let's look at some properties of matrices:

* Associativity:<br>
<center>$$\forall A \in \mathbb{R}^{m \times n}, B \in \mathbb{R}^{n \times p}, C \in \mathbb{R}^{p \times q} : (AB)C = A(BC) \tag{2.5}$$</center>

* Distributivity:<br>
<center>$$\forall A, B \in \mathbb{R}^{m \times n}, \; C, D \in \mathbb{R}^{n \times p} : (A+B)C = AC+BC \tag{2.6}$$</center><br>
<center>$$A(C+D) = AC+AD \tag{2.7}$$</center>

* Multiplication with the identity matrix:<br>
<center>$$\forall A \in \mathbb{R}^{m \times n} : I_m A = A I_n = A \tag{2.8}$$</center>
$\quad \ \ \  \text{Note that $I_{m} \neq I_{n}$ for $m\neq n$.}$


## 2-2) Inverse and Transpose
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 2.3 (Inverse)**<br>
Consider a square matrix **_A_** $\in \mathbb{R}^{n \times n}$.
Let matrix **_B_** $\in \mathbb{R}^{n \times n}$ have the property that **_AB_** = **_I_**$_{n}$ = **_BA_**.
**_B_** is called the **_inverse_** of **_A_**.
<div class="indented-paragraph" markdown="1">
**_A_**$^{-1}$
</div>

Not every matrix **_A_** possesses an inverse. 
If this inverse does exist, **_A_** is called **_regular/invertible/nonsingular_**, otherwise **_singular/noninvertible_**.
The inverse matrix is unique.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 2.4 (Transpose)**<br>
For **_A_** $\in \mathbb{R}^{m\times n}$ the matrix **_B_** $\in \mathbb{R}^{n \times m}$ with $b_{ij} = a_{ji}$ is called the transpose of **_A_**.
<div class="indented-paragraph" markdown="1">
**_B_** = **_A_**$^\top$
</div>
{% include end-box.html %}

The following are important properties of inverse and transpose:
$$\begin{align*}AA^{-1} &= I = A^{-1}A \tag{2.9} \\(AB)^{-1} &= B^{-1}A^{-1} \tag{2.10} \\(A + B)^{-1} &\neq A^{-1} + B^{-1} \tag{2.11} \\(A^{\top})^{\top} &= A \tag{2.12} \\(AB)^{\top} &= B^{\top}A^{\top} \tag{2.13} \\(A + B)^{\top} &= A^{\top} + B^{\top} \tag{2.14}\end{align*}$$

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 2.5 (Symmetric Matrix)**<br>
A matrix **_A_** $\in \mathbb{R}^{n\times n}$ is **_symmetric_** if **_A_** = **_A_**$^\top$.

Moreover, if **_A_** is invertible, then so is **_A_**$^\top$, and (**_A_**$^{-1}$)$^\top$ = (**_A_**$^\top$)$^{-1}$ =: **_A_**$^{-T}$.
{% include end-box.html %}

## 2-3) Multiplication by a Scalar
Let **_A_** $\in \mathbb{R}^{m \times n}$ and scalar $\lambda \in \mathbb{R}$. 
Then $\lambda$**_A_** = **_K_**, $K_{ij} = \lambda a_{ij}$. 
For $\lambda, \psi \in \mathbb{R}$, the following holds:

* Associativity:<br>
$$(\lambda\psi)C = \lambda(\psi C), \quad C \in \mathbb{R}^{m \times n}$$

* $\lambda(BC) = (\lambda B)C = B(\lambda C) = (BC)\lambda, \quad B \in \mathbb{R}^{m \times n}, C \in \mathbb{R}^{n \times k}$
$\text{Note that this allows us to move scalar values around.}$

* $(\lambda C)^{\top} = C^{\top}\lambda^{\top} = C^{\top}\lambda = \lambda C^{\top} \text{ since } \lambda = \lambda^{\top} \text{ for all } \lambda \in \mathbb{R}$
* Distributivity<br>
$(\lambda + \psi)C = \lambda C + \psi C, \quad C \in \mathbb{R}^{m \times n}$<br>
$\lambda(B+C) = \lambda B + \lambda C, \quad B, C \in \mathbb{R}^{m \times n}$
{% include end-box.html %}

# 3. Solving Systems of Linear Equations
{% include start-box.html class="math-box"%}
In the following, we will focus on solving systems of linear equations and provide an algorithm for finding the inverse of a matrix.

## 3-1) Particular and General Solution
Consider the system of equations

$$\begin{align}-2x_1 + 4x_2 - 2x_3 - 1x_4 + 4x_5 &= -3 \\4x_1 - 8x_2 + 3x_3 - 3x_4 + 1x_5 &= 2 \\1x_1 - 2x_2 + 1x_3 - 1x_4 + 1x_5 &= 0 \\1x_1 - 2x_2 + 0x_3 - 3x_4 + 4x_5 &= -1\end{align} \tag{3.1}$$

The system has four equations and five unknowns. Therefore, in general we would expect infinitely many solutions.
This equations can be converted into the compact matrix notation **_Ax=b_**.
Additionally, we can omit variables **_x_** and build the **_augmented matrix_** (in the form [**_A_**|**_b_**])

$$\left[
\begin{array}{rrrrr|r}
    -2 & 4 & -2 & -1 & 4 & -3 \\
    4 & -8 & 3 & -3 & 1 & 2 \\
    1 & -2 & 1 & -1 & 1 & 0 \\
    1 & -2 & 0 & -3 & 4 & -1
\end{array}
\right]$$

This augmented matrix can be converted into **_row-echelon form_**(REF) by Gaussian elimination.

$$\left[
\begin{array}{rrrrr|r}
    1 & -2 & 1 & -1 & 1 & 0 \\
    0 & 0 & 1 & -1 & 3 & -2 \\
    0 & 0 & 0 & 1 & -2 & 1 \\
    0 & 0 & 0 & 0 & 0 & 0
\end{array} \tag{3.2}
\right]$$

This REF can be reduced into **_reduced row-echelon form_** (RREF).

$$\left[
\begin{array}{rrrrr|r}
    1 & -2 & 0 & 0 & -2 & 2 \\
    0 & 0 & 1 & 0 & 1 & -1 \\
    0 & 0 & 0 & 1 & -2 & 1 \\
    0 & 0 & 0 & 0 & 0 & 0
\end{array} \tag{3.3}
\right]$$

Then, we can parametrize $x_1, x_3, x_4$ using $x_2$ and $x_5$. 

$$\begin{split} x_1 &= 2 + 2x_2 + 2x_5 \\ x_3 &= -1 - x_5 \\ x_4 &= 1 + 2x_5 \end{split} \Leftrightarrow \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \end{bmatrix} = \begin{bmatrix} 2 \\ 0 \\ -1 \\ 1 \\ 0 \end{bmatrix} + \begin{bmatrix} 2 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}x_2 + \begin{bmatrix} 2 \\ 0 \\ -1 \\ 2 \\ 1\end{bmatrix}x_5, \quad x_2, x_5 \in \mathbb{R} \tag{3.4}$$

The solution for $x_2,x_5 = 0$, [2 0 -1 1 0]$^\top$ , is called particular solution. 
The solution for **_Ax=0_**, [2 1 0 0 0]$^{T} x_2$ + [2 0 -1 2 1]$^{T} x_5$, is called homogeneous solution. 
And their sum is called general solution. 

$$\begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \end{bmatrix} \quad = \underbrace{\underbrace{\begin{bmatrix} 2 \\ 0 \\ -1 \\ 1 \\ 0 \end{bmatrix}}_{\text{particular solution}} + \quad \underbrace{\begin{bmatrix} 2 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}x_2 + \begin{bmatrix} 2 \\ 0 \\ -1 \\ 2 \\ 1\end{bmatrix}x_5}_{\text{homogeneous solution}}}_{\text{general solution}}, \quad x_2, x_5 \in \mathbb{R} \tag{3.5}$$

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 2.6 (Row-Echelon Form)**<br>
A matrix is in **_row-echelon form_** if

* All rows that contain only zeros are at the bottom of the matrix; correspondingly, all rows that contain at least one nonzero element are on top of rows that contain only zeros.
* Looking at nonzero rows only, the first nonzero number from the left (also called the pivot or the leading coefficient) is always strictly to the right of the pivot of the row above it.
* The variables corresponding to the pivots in the row-echelon form are called **_basic variables_** and the other variables are **_free variables_**.
{% include end-box.html %}

## 3-2) Algorithms for Solving a System of Linear Equations
In the following, approaches to solving a system of linear equations of the form **_Ax=b_** will be discussed. 

### 3-2-1) Moore-Penrose pseudo-inverse
Moore-Penrose pseudo-inverse is used to calculate inverse matrix of non-square matrix.
Under mild assumptions (**_A_** needs to have linearly indepenent columns) we can use the transformation 

$$Ax = b \Longleftrightarrow A^{\top}Ax = A^{\top}b \Longleftrightarrow x = (A^{\top}A)^{-1}A^{\top}b \tag{3.6}$$

$(A^{\top}A)^{-1}A^{\top}$ is the **_Moore-Penrose pseudo-inverse_** which corresponds to the minimum norm least-squares solution.
However, for reasons of numerical precision it is generally not recommended to compute the inverse or pseudo-inverse.
In the following, alternative approaches will be discussed. 

### 3-2-2) Gaussian elimination 
Gaussian elimination plays an important role when
<div class="indented-paragraph" markdown="1">
* computing determinants
* checking whether a set of vectors is linearly independent
* computing the inverse of a matrix
* computing the rank of a matrix
* determining a basis of a vector space
</div>

However, it is inappropriate for systems with millions of variables.
In practice, systems of many linear equations are solved indirectly, by either 
<div class="indented-paragraph" markdown="1">
stationary iterative methods
- the Richardson method
- the Jacobi method
- the Gaus-Seidel method
- the Successive over-relaxation method
</div>

<div class="indented-paragraph" markdown="1">
Krylov subspace methods
- Conjugate gradients
- Generalized minimal residual 
- Biconjugate gradients
</div>
{% include end-box.html %}