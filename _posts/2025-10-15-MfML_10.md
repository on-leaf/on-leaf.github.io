---
layout: single
title:  "5.Vector Calculus (1)"
category: "Mathematics for Machine Learning"
tag: [Machine Learning, Artificial Intelligence, Math]
header:
  teaser: #../images/2025-07-08-CT_6/image-20250708184503610.png
toc: true
toc_sticky: true
toc_label: CONTENTS
toc_icon: "fa-solid fa-seedling" # More icons: https://fontawesome.com/v6/search?ic=free
author_profile: false
sidebar:
    nav: "counts"
search: true # Change true to false if you don't want to have this article be searched 
redirect_from:
    - /Machine Learning/MfML_10
use_math: true
---

**[Reference]** <br>
$\bullet$ [MATHEMATICS FOR MACHINE LEARNING](https://mml-book.github.io/)
{: .notice--success}

# Introduction 
{% include start-box.html class="math-box"%}
Many algorithms in machine learning can be framed as optimization problems, where we seek to find the best model parameters that describe a set of data. 
To solve these problems, we need a systematic way to find the direction of steepest ascent, which is given by the gradient.

This chapter introduces the fundamental tools of vector calculus needed to compute these gradients. 
We assume that funcitons are differentiable.
{% include end-box.html %}

# 1.Differentiation of Univeariate Functions
{% include start-box.html class="math-box"%}
Let's start with the difference quotient of a univariate function $y=f(x), x,y \in \mathbb{R}$.

<figure style="display: flex; flex-direction: column; align-items: center; margin-top: 0.5em; margin-bottom: 0.5em;">
  <img src="../images/2025-10-15-MfML_10/Fig_1.png" alt="" 
       style="width: 30%; height: auto;">
   <figcaption style="font-size: 20px; margin-top: -0.5em;">
   Fig.5.1. The average incline of a function $f$ between $x_0$ and $x_0 + \delta x$ is the incline of the secant (blue) through $f(x_0)$ and $f(x_0 + \delta x)$ and given by $\delta y/\delta x$.
   </figcaption>
</figure> 

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 5.1 (Difference Quotient)**<br>
The _difference quotient_ computes the slope of the secant line through two points on the graph of $f$.

$$\frac{\delta y}{\delta x} := \frac{f(x+\delta x) - f(x)}{\delta x} \tag{5.1}$$

It can also be considered the average slope of $f$ between $x$ and $x + \delta x$ if we assume $f$ to be a linear function. 
For $\delta x \rightarrow 0$, we obtain the tangent of $f$ at $x$ which is the derivate of $f$ at $x$, if $f$ is differentiable.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 5.2 (Derivative)**<br>
For $h \gt 0$ the derivataive of $f$ at $x$ is defined as the below and the secant in Fig.5.1 becomes a tangent.

$$\frac{\mathrm{d}f}{\mathrm{d}x} := \lim_{h \to 0} \frac{f(x+h) - f(x)}{h} \tag{5.2}$$

The derivative of $f$ points in the direction of steepest ascent of $f$.
{% include end-box.html %}

## 1-1) Taylor Series
The Taylor series is a representation of a funciton $f$ as an infinite sum of terms.
These terms are determined using derivatives of $f$ evaluated at $x_0$.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 5.3 (Taylor Polynomial)**<br>
The _Taylor polynomial_ of degree $n$ of $f: \mathbb{R} \rightarrow \mathbb{R}$ at $x_0$ is defined as below where $f^{(k)}(x_0)$ is the $k$th derivative of $f$ at $x_0$ and $\frac{f^{(k)}(x_0)}{k!}$ are the coefficients of the polynomial.

$$T_n(x) := \sum_{k=0}^{n} \frac{f^{(k)}(x_0)}{k!} (x - x_0)^k, \tag{5.3}$$

{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 5.4 (Taylor Series)**<br>
For a smooth function $f \in \mathcal{C}^{\infty}, f:\mathbb{R} \rightarrow \mathbb{R}$, the _Taylor series_ of $f$ at $x_0$ is defined as below.

$$T_{\infty}(x) = \sum_{k=0}^{\infty} \frac{f^{(k)}(x_0)}{k!} (x - x_0)^k. \tag{5.4}$$

If $f(x)=T_{\infty}(x)$, then $f$ is called **_analytic_**.
For $x_0 = 0$, we obtain the _Maclaurin series_ as a special instance of the Taylor series.
{% include end-box.html %}

## 1-2) Differentiation Rules
The basic differentiation rules are below.

$$\begin{align}
\text{Product rule:} \quad & (f(x)g(x))' = f'(x)g(x) + f(x)g'(x) \tag{5.5} \\
\text{Quotient rule:} \quad & \left(\frac{f(x)}{g(x)}\right)' = \frac{f'(x)g(x) - f(x)g'(x)}{(g(x))^2} \tag{5.6} \\
\text{Sum rule:} \quad & (f(x) + g(x))' = f'(x) + g'(x) \tag{5.7} \\
\text{Chain rule:} \quad & (g(f(x)))' = (g \circ f)'(x) = g'(f(x))f'(x) \tag{5.8}
\end{align}
$$

Here, $g \circ f$ denotes function composition $x \mapsto f(x) \mapsto g(f(x))$.
{% include end-box.html %}

# 2.Partial Differentiation and Gradients
{% include start-box.html class="math-box"%}
Let's consider the general case where the function $f$ depends on one or more variables $\vec{x} \in \mathbb{R}^n$. 

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 5.5 (Partial Derivative)**<br>
For a function $f: \mathbb{R}^{n} \rightarrow \mathbb{R}, \vec{x} \mapsto f(\vec{x}), \vec{x} \in \mathbf{R}^n$ of $n$ variables $x_1, \dots, x_n$, we define the _partial derivatives_ as 

$$\begin{align}
\frac{\partial f}{\partial x_1} &= \lim_{h \to 0} \frac{f(x_1 + h, x_2, \dots, x_n) - f(\mathbf{x})}{h} \nonumber \\
& \ \ \vdots \tag{5.9} \\
\frac{\partial f}{\partial x_n} &= \lim_{h \to 0} \frac{f(x_1, \dots, x_{n-1}, x_n + h) - f(\mathbf{x})}{h} \nonumber
\end{align}$$

and collect them inthe row vector

$$\nabla_{\vec{x}} f = \text{grad} f = \frac{\mathrm{d}f}{\mathrm{d}\vec{x}} = \begin{bmatrix} \frac{\partial f(\vec{x})}{\partial x_1} & \frac{\partial f(\vec{x})}{\partial x_2} & \dots & \frac{\partial f(\vec{x})}{\partial x_n} \end{bmatrix} \in \mathbb{R}^{1 \times n}, \tag{5.10}$$

where $n$ is the number of variables and 1 is the dimension of the image/range/codomain of $f$.

Here, we defined the $\vec{x} = [x_1, \dots, x_n]^{\top} \in \mathbf{R}^{n}$ as the column vector and the **_gradient_** of $f$ (also called **_Jacobian_**) of (5.10) as the row vector.
<div class="indented-paragraph" markdown="1">
This notation is called **_numerator layout_**.
</div>

The reason why we define the gradient vector as a row vector is twofold:
<div class="indented-paragraph" markdown="1">
- We can consistently generalize the gradient to vector-valued functions $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$ (then the gradient becomes a matrix).
- We can immediately apply the multi-variate chain rule without paying attention to the dimension of the gradient.
</div>

## 2-1) Basic Rules of Partial Differentiation
The basic differentiation rules with respect to vectors $\vec{x} \in $\mathbb{R}^n$ are below. 
We need to pay attention: Our gradients now involve voctors and matrices, and matrix multiplication is not commutative, the order matters.

$$\begin{align}
\text{Product rule:} \quad & \frac{\partial}{\partial \mathbf{x}}(f(\mathbf{x})g(\mathbf{x})) = \frac{\partial f}{\partial \mathbf{x}}g(\mathbf{x}) + f(\mathbf{x})\frac{\partial g}{\partial \mathbf{x}} \tag{5.11} \\
\text{Sum rule:} \quad & \frac{\partial}{\partial \mathbf{x}}(f(\mathbf{x}) + g(\mathbf{x})) = \frac{\partial f}{\partial \mathbf{x}} + \frac{\partial g}{\partial \mathbf{x}} \tag{5.12} \\
\text{Chain rule:} \quad & \frac{\partial}{\partial \mathbf{x}}(g \circ f)(\mathbf{x}) = \frac{\partial}{\partial \mathbf{x}}(g(f(\mathbf{x}))) = \frac{\partial g}{\partial f}\frac{\partial f}{\partial \mathbf{x}} \tag{5.13}
\end{align}$$

{% include end-box.html %}

## 2-2) Chain Rule


{% include end-box.html %}















<div class="indented-paragraph" markdown="1">

</div>

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition . ()**<br>
{% include end-box.html %}


{% include start-box.html class="math-box"%}

{% include end-box.html %}