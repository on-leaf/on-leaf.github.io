---
layout: single
title:  "4.Dimensionality Reduction with Principal Component Analysis (1)"
category: "Mathematics for Machine Learning"
tag: [Machine Learning, Artificial Intelligence, Math]
header:
  teaser: #../images/2025-07-08-CT_6/image-20250708184503610.png
toc: true
toc_sticky: true
toc_label: CONTENTS
toc_icon: "fa-solid fa-seedling" # More icons: https://fontawesome.com/v6/search?ic=free
author_profile: false
sidebar:
    nav: "counts"
search: true # Change true to false if you don't want to have this article be searched 
redirect_from:
    - /Machine Learning/MfML_8
use_math: true
---

**[Reference]** <br>
$\bullet$ [MATHEMATICS FOR MACHINE LEARNING](https://mml-book.github.io/)
{: .notice--success}

# Introduction 
{% include start-box.html class="math-box"%}
Working directly with high-dimensional data, such as images, comes with some difficulties: hard to analyze/interpretation/visualization and storage of the data vectors can be expensive.
However, high-dimensional data is often overcomplete.
<div class="indented-paragraph" markdown="1">
Dimensionality of data can be reduced and it allows us to work with a more compact representation of the data, ideally without losing information.
</div>
In this chapter, we will discuss **_principal component analysis_** (PCA), an algorithm for linear dimensionality reduction.
{% include end-box.html %}

# 1.Problem Setting
{% include start-box.html class="math-box"%}
In PCA, we are interested in finding projections $\tilde{\vec{x}}_n$ of data points $\vec{x}_n$ that are as similar to the original data points as possible, but which have a significantly lower intrinsic dimensionality. 

Consider an i.i.d. dataset $$\mathcal{X} = \{\vec{x}_1, \dots, \vec{x}_N\}$$ whose mean is $\vec{0}$, where each $$\vec{x}_n \in \mathbb{R}^D$$, and its corresponding data matrix can be written as

$$\mathbf{X} = \begin{pmatrix} \vec{x}_1 & \dots & \vec{x}_N \end{pmatrix} = \begin{pmatrix} x_{1,1} & x_{1,2} & \dots & x_{1,N} \\ x_{2,1} & x_{2,2} & \dots & x_{2,N} \\ \vdots & \vdots & \ddots & \vdots \\ x_{D,1} & x_{D,2} & \dots & x_{D,N} \end{pmatrix} \in \mathbb{R}^{D \times N}, \tag{1.1}$$

and its covariance matrix $(\mathbf{\Sigma} := \frac{1}{N} \sum_{n=1}^{N} (\vec{x}_n - \bar{\vec{x}})(\vec{x}_n - \bar{\vec{x}})^\top)$ can be written as

$$ \mathbf{S} = \frac{1}{N} \sum_{n=1}^{N} \vec{x}_n \vec{x}_n^\top. \tag{1.2}$$

Furthermore, we assume there exists a low-dimensional compressed representation (code)

$$ \vec{z}_n = \mathbf{B}^\top \vec{x}_n \in \mathbb{R}^M \tag{1.3} $$

of $\vec{x}_n$, where we define the projection matrix


$$ \mathbf{B} := [\vec{b}_1, \dots, \vec{b}_M] \in \mathbb{R}^{D \times M}. \tag{1.4} $$

We assume that the columns of $\mathbf{B}$ are orthonormal (Definition 3.7) so that $\vec{b}_i^\top \vec{b}_j = 0$ if and only if $i \neq j$ and $\vec{b}_i^\top \vec{b}_i = 1$. 
We seek an $M$-dimensional subspace $U \subseteq \mathbb{R}^D$, $\text{dim}(U) = M < D$ onto which we project the data. 
We denote the projected data by $\tilde{\vec{x}}_n \in U$, and their coordinates (with respect to the basis vectors $\vec{b}_1, \dots, \vec{b}_M$ of $U$) by $\vec{z}_n$. 
Our aim is to find projections $\tilde{\vec{x}}_n \in \mathbb{R}^D$ (or equivalently the codes $\vec{z}_n$ and the basis vectors $\vec{b}_1, \dots, \vec{b}_M$) so that they are as similar to the original data $\vec{x}_n$ and minimize the loss due to compression.

Figure 1.2 illustrates the setting we consider in PCA, where $\vec{z}$ represents the lower-dimensional representation of the compressed data $\tilde{\vec{x}}$ and plays the role of a bottleneck, which controls how much information can flow between $\vec{x}$ and $\tilde{\vec{x}}$. 

<figure style="display: flex; flex-direction: column; align-items: center; margin-top: 0.5em; margin-bottom: 0.5em;">
  <img src="../images/2025-10-04-MfML_8/Fig_1.png" alt="" 
       style="width: 40%; height: auto;">
   <figcaption style="font-size: 20px; margin-top: -0.5em;">
   Fig.1.1 Graphical illustration of PCA.
   </figcaption>
</figure> 

In PCA, we consider a linear relationship between the original data $\vec{x}$ and its low-dimensional code $\vec{z}$ so that $\vec{z} = \mathbf{B}^\top\vec{x}$ and $\tilde{\vec{x}} = \mathbf{B}\vec{z}$ for a suitable matrix $\mathbf{B}$. 
We can interpret the arrows in Figure 1.1 as a pair of operations representing encoders and decoders. 
The linear mapping represented by $\mathbf{B}$ can be thought of as a decoder, which maps the low-dimensional code $\vec{z} \in \mathbb{R}^M$ back into the original data space $\mathbb{R}^D$. 
Similarly, $\mathbf{B}^\top$ can be thought of an encoder, which encodes the original data $\vec{x}$ as a low-dimensional (compressed) code $\vec{z}$.

{% include end-box.html %}

# 2.Maximum Variance Perspective 
{% include start-box.html class="math-box"%}
If we interpret information content in the data as how "space filling" the dataset is, then we can describe the information contained in the data by looking at the spread of the data.
<div class="indented-paragraph" markdown="1">
The variance is an indicator of the spread of the data.<br>
PCA is dimensionality reduction algorithm that maximizes the variance in the low-dimensional representation of the data.
</div>

Our aim is to find a matrix $\mathbf{B}$ that retains as much information as possible when compressing data by projecting it onto the subspace spanned by the columns $\vec{b}_1, \dots, \vec{b}_M$ of $\mathbf{B}$.
<div class="indented-paragraph" markdown="1">
Retaining most information after data compression is equivalent to capturing the largest amount of variance in the low-dimensional code.
</div>

## 2-1) Direction with Maximal Variance
We maximize the variance of the low-dimensional code using a sequential approach. 

We start by seeking a single vector $\vec{b}_1 \in \mathbb{R}^{D}$ that maximizes the variance of the projected data.
<div class="indented-paragraph" markdown="1">
Maximize the variance of the first coordinate $z_1$ of $\vec{z} \in \mathbb{R}^M$.
</div>

Then,

$$V_1 := \mathbb{V}[z_1] = \frac{1}{N} \sum_{n=1}^{N} z_{1n}^2 \text{ where } z_{1n} = \vec{b}_1^{\top} \vec{x}_n \tag{2.1}$$

is maximized.

It is the coordinate of the orthogonal projection of $\vec{x}_n$ onto the one-dimensional subspace spanned by $\vec{b}_1$.
From (2.1), we can obtain 

$$\begin{align}
V_1 &= \frac{1}{N} \sum_{n=1}^{N} (\vec{b}_1^\top\vec{x}_n)^2 = \frac{1}{N} \sum_{n=1}^{N} \vec{b}_1^\top\vec{x}_n\vec{x}_n^\top\vec{b}_1 \tag{2.2a} \\
&= \vec{b}_1^\top \left( \frac{1}{N} \sum_{n=1}^{N} \vec{x}_n\vec{x}_n^\top \right) \vec{b}_1 = \vec{b}_1^\top\mathbf{S}\vec{b}_1, \tag{2.2b}
\end{align}$$

where $\mathbf{S}$ is the data covariance matrix defined in (1.1).
We restrict all solutions to $\Vert \vec{b}_1 \Vert^2 = 1$.
Then $\vec{b}_1$ can be found by the 

$$\begin{array}{l}
\displaystyle\max_{\vec{b}_1} \vec{b}_1^\top \mathbf{S} \vec{b}_1 \\
\text{subject to } \Vert\vec{b}_1\Vert^2 = 1.
\end{array} \tag{2.3}$$

It can be solved by the Lagrangian 

$$\mathcal{L}(\vec{b}_1, \lambda) = \vec{b}_1^\top \mathbf{S} \vec{b}_1 + \lambda_1(1 - \vec{b}_1^\top \vec{b}_1) \tag{2.4}$$

and the partial derivatives of $\mathcal{L}$ with repect to $\vec{b}_1$ and $\lambda_1$ are

$$\frac{\partial\mathcal{L}}{\partial\vec{b}_1} = 2\vec{b}_1^\top\mathbf{S} - 2\lambda_1\vec{b}_1^\top, \quad \frac{\partial\mathcal{L}}{\partial\lambda_1} = 1 - \vec{b}_1^\top\vec{b}_1. \tag{2.5}$$

Setting these partial derivatives to $\vec{0}$ gives us the relations 

$$\begin{align}
\mathbf{S}\vec{b}_1 &= \lambda_1\vec{b}_1, \tag{2.6} \\
\vec{b}_1^\top\vec{b}_1 &= 1. \tag{2.7}
\end{align}$$

We can see that $\vec{b}_1$ is an eigenvector of the data covariance matrix $\mathbf{S}$, and the Lagrange multiplier $\lambda_1$ plays the role ot the corresponding eigenvalue.
This eigenvector property allows us to rewrite our variance objective as 

$$V_1 = \vec{b}_1^\top \mathbf{S} \vec{b}_1 = \lambda_1 \vec{b}_1^\top \vec{b}_1 = \lambda_1, \tag{2.8}$$

i.e., the variance of the data projected onto a one-dimensional subspace equals the eigenvalue that is associated with the basis vector $\vec{b}_1$ that spans this subspace.
Therefore, to maximize the variance of the low-dimensional code, we choose the basis vector associated with the largest eigenvalue of the data covariance matrix. 
This eigenvector is called the **_first principal component_**. 
We can determine the effect/contribution of the principal component $\vec{b}_1$ in the original data space by mapping the coordinate $z_{1n}$ back into data space, which gives us the projected data point 

$$\tilde{\vec{x}}_n = \vec{b}_1 z_{1n} = \vec{b}_1 \vec{b}_1^\top \vec{x}_n \in \mathbb{R}^D \tag{2.9}$$

in the orginal data space. 

## 2-2) M-dimensional Subspace with Maximal Variance

{% include end-box.html %}















<div class="indented-paragraph" markdown="1">

</div>

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition . ()**<br>
{% include end-box.html %}


{% include start-box.html class="math-box"%}

{% include end-box.html %}