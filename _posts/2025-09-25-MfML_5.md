---
layout: single
title:  "2.Analytic Geometry (2)"
category: "Mathematics for Machine Learning"
tag: [Machine Learning, Artificial Intelligence, Math]
header:
  teaser: #../images/2025-07-08-CT_6/image-20250708184503610.png
toc: true
toc_sticky: true
toc_label: CONTENTS
toc_icon: "fa-solid fa-seedling" # More icons: https://fontawesome.com/v6/search?ic=free
author_profile: false
sidebar:
    nav: "counts"
search: true # Change true to false if you don't want to have this article be searched 
redirect_from:
    - /Machine Learning/MfML_5
use_math: true
---

**[Reference]** <br>
$\bullet$ [MATHEMATICS FOR MACHINE LEARNING](https://mml-book.github.io/)
{: .notice--success}

# Introduction 
{% include start-box.html class="math-box"%}
We will add some geometric interpretation and intuition to concenpts of vectors, vector spaces, and linear mappings. 
To look at geometric vectors and compute their lengths and distances or angles between two vectors, we need to equip the vector space with an inner product that induces the geometry of the vector space. 

- Inner product and their corresponding norms and metrics
    <div class="indented-paragraph" markdown="1">
    Similarity and distances for support vector machine in Ch.12
    </div>
- Lengths and angles between vectors
    <div class="indented-paragraph" markdown="1">
    Orthogonal projections for principal component analysis in Ch.10<br>
    Regression via maximum likelihood estimation in Ch.9
    </div>
{% include end-box.html %}

# 5. Orthonormal Basis
{% include start-box.html class="math-box"%}
We will discuss the special case where the basis vectors are orthogonal to each other and where the length of each basis vector is 1.
This basis is called **_orthonormal basis_**.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 3.9 (Orthonormal Basis)**<br>
Consisder $n$-dimensional vector space $V$ and a basis $\{ \vec{b}_1, \dots, \vec{b}_n \}$ of V.
If

$$<\vec{b}_i, \vec{b}_j> = 0 \text{ for } i \neq j \tag{5.1}$$

for all $i,j=1. \dot, n$ then the basis is called an **_orthogonal basis_**.

Additionally, 

$$<\vec{b}_i, \vec{b}_i> = 1 \tag{5.2}$$

is satisfied, the basis is called an **_orthonormal basis_** (ONB).

From unorthogonal basis, we can struct orthogonal basis by **_Gram-Schmidt process_**.
{% include end-box.html %}
{% include end-box.html %}


# 6. Orthogonal Complement
{% include start-box.html class="math-box"%}
Consider a $D$-dimensional vector space $V$ and an $M$-dimensional subspace $U \subseteq V$.

Then orthogonal complement of subspace $U$, $U^{\top}$, is defined as
- $(D-M)$-dimensional subspace of $V$
- Contains all vectors in $V$ that are orthogonal to every vector in $U$

Furthermore, $U \cap U^{\top} = \{ \vec{0} \}$ so that any vector $\vec{x} \in V$ can be uniquely decomposed into 

$$\vec{x} = \sum_{m=1}^{M} \lambda_m \vec{b}_m + \sum_{j=1}^{D-M} \psi_j \vec{b}_j^{\perp}, \quad \lambda_m, \psi_j \in \mathbb{R}, \tag{5.3}$$

where $(\vec{b}_1, \dots, \vec{b}_M)$ in a basis of $U$ and $(\vec{b}_1^{\top}, \dots, \vec{b}^{\top}_{D-M})$ is a basis of $U^{\top}$.

{% include end-box.html %}














<div class="indented-paragraph" markdown="1">

</div>

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition . ()**<br>
{% include end-box.html %}


{% include start-box.html class="math-box"%}

{% include end-box.html %}