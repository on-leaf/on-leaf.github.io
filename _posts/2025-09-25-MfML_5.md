---
layout: single
title:  "2.Analytic Geometry (2)"
category: "Mathematics for Machine Learning"
tag: [Machine Learning, Artificial Intelligence, Math]
header:
  teaser: #../images/2025-07-08-CT_6/image-20250708184503610.png
toc: true
toc_sticky: true
toc_label: CONTENTS
toc_icon: "fa-solid fa-seedling" # More icons: https://fontawesome.com/v6/search?ic=free
author_profile: false
sidebar:
    nav: "counts"
search: true # Change true to false if you don't want to have this article be searched 
redirect_from:
    - /Machine Learning/MfML_5
use_math: true
---

**[Reference]** <br>
$\bullet$ [MATHEMATICS FOR MACHINE LEARNING](https://mml-book.github.io/)
{: .notice--success}

# Introduction 
{% include start-box.html class="math-box"%}
We will add some geometric interpretation and intuition to concenpts of vectors, vector spaces, and linear mappings. 
To look at geometric vectors and compute their lengths and distances or angles between two vectors, we need to equip the vector space with an inner product that induces the geometry of the vector space. 

- Inner product and their corresponding norms and metrics
    <div class="indented-paragraph" markdown="1">
    Similarity and distances for support vector machine in Ch.12
    </div>
- Lengths and angles between vectors
    <div class="indented-paragraph" markdown="1">
    Orthogonal projections for principal component analysis in Ch.10<br>
    Regression via maximum likelihood estimation in Ch.9
    </div>
{% include end-box.html %}

# 5. Orthonormal Basis
{% include start-box.html class="math-box"%}
We will discuss the special case where the basis vectors are orthogonal to each other and where the length of each basis vector is 1.
This basis is called **_orthonormal basis_**.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 3.9 (Orthonormal Basis)**<br>
Consisder $n$-dimensional vector space $V$ and a basis $\{ \vec{b}_1, \dots, \vec{b}_n \}$ of V.
If

$$<\vec{b}_i, \vec{b}_j> = 0 \text{ for } i \neq j \tag{5.1}$$

for all $i,j=1. \dot, n$ then the basis is called an **_orthogonal basis_**.

Additionally, 

$$<\vec{b}_i, \vec{b}_i> = 1 \tag{5.2}$$

is satisfied, the basis is called an **_orthonormal basis_** (ONB).

From unorthogonal basis, we can struct orthogonal basis by **_Gram-Schmidt process_**.
{% include end-box.html %}
{% include end-box.html %}


# 6. Orthogonal Complement
{% include start-box.html class="math-box"%}
Consider a $D$-dimensional vector space $V$ and an $M$-dimensional subspace $U \subseteq V$.

Then orthogonal complement of subspace $U$, $U^{\top}$, is defined as
- $(D-M)$-dimensional subspace of $V$
- Contains all vectors in $V$ that are orthogonal to every vector in $U$

Furthermore, $U \cap U^{\top} = \{ \vec{0} \}$ so that any vector $\vec{x} \in V$ can be uniquely decomposed into 

$$\vec{x} = \sum_{m=1}^{M} \lambda_m \vec{b}_m + \sum_{j=1}^{D-M} \psi_j \vec{b}_j^{\perp}, \quad \lambda_m, \psi_j \in \mathbb{R}, \tag{5.3}$$

where $$(\vec{b}_1, \dots, \vec{b}_M)$$ in a basis of $U$ and $$(\vec{b}_1^{\top}, \dots, \vec{b}^{\top}_{D-M})$$ is a basis of $U^{\top}$.

For a linear mapping represented by a matrix $A \in \mathbb{R}^{n \times n}$, the soluiton of $A \vec{x} = \vec{0}$ is row$(A)^{\perp}$.
<div class="indented-paragraph" markdown="1">
row$(A)^{\perp}$: row space of $A$
</div>
In other words, row$(A)^{\perp}$ = ker$(A)$.
{% include end-box.html %}

# 7. Inner Product of Functions
{% include start-box.html class="math-box"%}
We will look at inner products of functions. 
The inner products we discussed so far were defined for vectors with a finite number of entreis. 
We can think of a vector $\vec{x} \in \mathbb{R}^n$ as a function with $n$ function values.
The concept of an inner product can be generalized to continuous-valued functions.

An inner product of two functions $u: \mathbb{R} \rightarrow \mathbb{R}$ and $v: \mathbb{R} \rightarrow \mathbb{R}$ can be defined as the definite integral

$$\langle u, v \rangle := \int_{a}^{b} u(x)v(x)dx, \quad a,b \lt \infty \tag{7.1}$$

As with our usual inner product, if it evaluates to 0, the functions $u$ and $v$ are orthogonal.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
The collection of functions 

$$\{ 1, \cos(x), \cos(2x), \cos(3x), \dots \} \tag{7.2}$$

is orthogonal if we integrate from $-\pi$ to $\pi$.
<div class="indented-paragraph" markdown="1">
If any pair of functions are orthogonal to each other
</div>

The collection of functions in (7.2) spans a large subspace of the functions that are even and periodic on $[-\pi, \pi)$, and projecting functions onto this subspace is the fundamental idea behind Fourier series.
{% include end-box.html %}
{% include end-box.html %}


# 8. Orthogonal Projections
{% include start-box.html class="math-box"%}
In machine learning, we often deal with high-dimensional data.
<div class="indented-paragraph" markdown="1">
High-dimensional data is often hard to analyze or visualize
</div>
However, they often possesses the property that only a few dimensions contain most information, and most other dimensions are not essential for key propoerties of the data.
When we compress high-demensional data, we will lose information.
<div class="indented-paragraph" markdown="1">
To minimize compression loss, we ideally find the most informative dimensions in the data
</div>
We can project the high-dimensional data onto a lower-dimensional feature space and work in lower-dimensional space to learn more about the dataset and extract relevant patterns.
<div class="indented-paragraph" markdown="1">
Ex. Machine learning - PCA(principal component analysis), Deep learning
</div>
For a given lower-dimensinoal subspace, orthogonal projections of high-dimensional data are most effective for minimization of the difference/error between the original data.
Let's see what a projection actually is first.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 3.10 (projection)**<br>
Let $V$ be a vector space and $U \subseteq V$ a subspace of $V$.
A linear maapping $\pi: V \rightarrow U$ is called a **_projection_** if $\pi^2 = \pi \circ \pi = \pi$.

<div class="indented-paragraph" markdown="1">
The key condition for this transformation to be called a projection is that the result of applying it twice is the same as applying it once.
</div>

Since linear mappings can be expressed by transformation matrices, projection also can be expressed as matrices, $P_\pi$.
<div class="indented-paragraph" markdown="1">
$P_{\pi}^2 = P_{\pi}$
</div>
{% include end-box.html %}

In the following, we will derive orthogonal projections of vectors in the inner product space $(\mathbb{R}^n, \langle \cdot, \cdot \rangle)$ onto subspaces.
We assume the dot product $\langle \vec{x}, \vec{y} \rangle = \vec{x}^{\top}\vec{y}$ as the inner product.

## 8-1) Projection onto One-Dimensional Subspaces (Lines)
Assume we are given a line (one-dimensional subspace) through the origin with basis vector $\vec{b} \in \mathbb{R}^n$.
<div class="indented-paragraph" markdown="1">
$U \subseteq \mathbb{R}^n$ spanned by $\vec{b}$
</div>

When we project $\vec{x} \in \mathbb{R}^n$ onto $U$, we seek the vector $\pi_U (\vec{x}) \in U$ that is closest to $\vec{x}$.

<figure style="display: flex; flex-direction: column; align-items: center; margin-top: 0.5em; margin-bottom: 0.5em;">
  <img src="../images/2025-09-25-MfML_5/Fig_1.png" alt="" 
       style="width: 30%; height: auto;">
   <figcaption style="font-size: 20px; margin-top: -0.5em;">
   Fig.8.1 Projection of $\vec{x} \in \mathbb{R}^2$ onto a subspace $U$ with basis vector $\vec{b}$.
   </figcaption>
</figure> 

1.Since projection $\pi_U (\vec{x})$ is closest to $\vec{x}$, the distance $\Vert \vec{x} - \pi_U(\vec{x}) \Vert$ should be minimum.
<div class="indented-paragraph" markdown="1">
$\pi_U (\vec{x}) - \vec{x}$ is orthogonal to $U$, and therefore the basis vector $\vec{b}$
</div>

$$\langle \pi_U(\vec{x}-\vec{x}, \vec{b}) \rangle = 0$$

2.The projection $\pi_U (\vec{x})$ of $\vec{x}$ onto $U$ must be an element of $U$.
    
$$\pi_U (\vec{x}) = \lambda \vec{b}$$

3.Finding coordinate $\lambda$

$$\langle \vec{x} - \pi_U(\vec{x}), \vec{b} \rangle = 0 \stackrel{\pi_U(\vec{x})=\lambda \vec{b}}{\Longleftrightarrow} \langle \vec{x} - \lambda \vec{b}, \vec{b} \rangle = 0 \tag{7.3}$$

$$\langle \vec{x}, \vec{b} \rangle - \lambda \langle \vec{b}, \vec{b} \rangle = 0 \Longleftrightarrow \lambda = \frac{\langle \vec{x}, \vec{b} \rangle}{\langle \vec{b}, \vec{b} \rangle} = \frac{\langle \vec{b}, \vec{x} \rangle}{\Vert \vec{b}\Vert^2} = \frac{\vec{b}^\top \vec{x}}{\vec{b}^\top \vec{b}} = \frac{\vec{b}^\top \vec{x}}{\Vert \vec{b}\Vert^2} \tag{7.4}$$

4.Finding the projection point $\pi_U (\vec{x}) \in U$.




{% include end-box.html %}









<div class="indented-paragraph" markdown="1">

</div>

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition . ()**<br>
{% include end-box.html %}


{% include start-box.html class="math-box"%}

{% include end-box.html %}