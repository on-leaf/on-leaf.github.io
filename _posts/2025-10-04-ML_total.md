---
layout: single
title:  "ML total"
category: "Machine Learning"
tag: [Machine Learning, Artificial Intelligence]
header:
  teaser: #../images/2025-07-08-CT_6/image-20250708184503610.png
toc: true
toc_sticky: true
toc_label: CONTENTS
toc_icon: "fa-solid fa-seedling" # More icons: https://fontawesome.com/v6/search?ic=free
author_profile: false
sidebar:
    nav: "counts"
search: true # Change true to false if you don't want to have this article be searched 
redirect_from:
    - /Machine Learning/ML_1
use_math: true
---

**[Reference]** <br>
$\bullet$ [Machine Learning Q and AI](https://sebastianraschka.com/books/ml-q-and-ai/#table-of-contents)
{: .notice--success}

# W1-2
{% include start-box.html class="math-box"%}
## 1.Embeddings
- Embedding vectors or embeddings for short, encod relatively high-dimensional data into relatively low-dimensional vectors. 
    <div class="indented-paragraph" markdown="1">
    We can apply embedding methods to create a $\underbrace{\text{dense and continuous}}_{\text{Two characteristics of embedding}}$ vector.<br>
        - Dense: Embedding transform sparse vector into dense vector.<br>
        - Continuous: Similar data are embedded into similar vector by embedding.
    </div>

- Similar inputs are mapped to nearby locations (similarity is preserved).

- Mainly used for similarity and distance computations.

## 2.Latent space
The space into which embedding vectors are mapped (feature space).<br>
Preservation of similarity is not necessary condition for latent space.

## 3.Representation
A representation is an encoded, typically intermediate form of an input. <br>
For instance, an embedding vector or vector in the latent space is a representation of the input. <br>
However, simpler form of encoded vectors like one-hot encoded vectors also can be considered representation.

{% include end-box.html %}

# W2-1
{% include start-box.html class="math-box"%}

## 1.Neural Network
What we do in deep learning: Transform (refine) input features into better embedding.

Essential components of a Neural Network: **Linear transformation** and **Activation function**.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
1) Linear transformation

$$y=Wx+b$$

Projects the data into a different coordinate system, continually transforming the feature space.

{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
2) Activation function

$$\hat{y} = \sigma(y)$$

Injects nonlinearity, enabling the network to approximate complex functions that simple linear models cannot.

Ex) ReLU: $\text{ReLU}(x) = \max(0,x)$

{% include end-box.html %}

## 2.Universal Approximation Theorem (UAT)
A neural network with a single hidden layer, provided it has a sufficient number of neurons (capacity), can approximate any continuous function to a desired level of accuracy. 


## 3.Deep learning
Essential components of Deep learning: **Neural network**, **Loss function** and **Optimizer**.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
Softmax function<br>
: Usually used for final layer of classification model. 

Each class's output value is transformed into a number  between 0 and 1.<br>
Total sum of all classes should be 1. 

{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
Cross-entropy Loss<br>
: To calculate the dissimilarity between the predicted probability distribution and the true probability distribution.

It is usually used in classification model. 

{% include end-box.html %}

{% include end-box.html %}





<div class="indented-paragraph" markdown="1">

</div>

{% include start-box.html class="math-box-inner" font_size="0.8em"%}

{% include end-box.html %}


{% include start-box.html class="math-box"%}

{% include end-box.html %}