---
layout: single
title:  "ML total"
category: "Machine Learning"
tag: [Machine Learning, Artificial Intelligence]
header:
  teaser: #../images/2025-07-08-CT_6/image-20250708184503610.png
toc: true
toc_sticky: true
toc_label: CONTENTS
toc_icon: "fa-solid fa-seedling" # More icons: https://fontawesome.com/v6/search?ic=free
author_profile: false
sidebar:
    nav: "counts"
search: true # Change true to false if you don't want to have this article be searched 
redirect_from:
    - /Machine Learning/ML_1
use_math: true
---

**[Reference]** <br>
$\bullet$ [Machine Learning Q and AI](https://sebastianraschka.com/books/ml-q-and-ai/#table-of-contents)
{: .notice--success}

# W1-2
{% include start-box.html class="math-box"%}
## 1.Embeddings
- Embedding vectors or embeddings for short, encod relatively high-dimensional data into relatively low-dimensional vectors. 
    <div class="indented-paragraph" markdown="1">
    We can apply embedding methods to create a $\underbrace{\text{dense and continuous}}_{\text{Two characteristics of embedding}}$ vector.<br>
        - Dense: Embedding transform sparse vector into dense vector.<br>
        - Continuous: Similar data are embedded into similar vector by embedding.
    </div>

- Similar inputs are mapped to nearby locations (similarity is preserved).

- Mainly used for similarity and distance computations.

## 2.Latent space
The space into which embedding vectors are mapped (feature space).<br>
Preservation of similarity is not necessary condition for latent space.

## 3.Representation
A representation is an encoded, typically intermediate form of an input. <br>
For instance, an embedding vector or vector in the latent space is a representation of the input. <br>
However, simpler form of encoded vectors like one-hot encoded vectors also can be considered representation.

{% include end-box.html %}

# W2-1
{% include start-box.html class="math-box"%}

## 1.Neural Network
What we do in deep learning: Transform (refine) input features into better embedding.

Essential components of a Neural Network: **Linear transformation** and **Activation function**.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
1) Linear transformation

$$y=Wx+b$$

Projects the data into a different coordinate system, continually transforming the feature space.

{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
2) Activation function

$$\hat{y} = \sigma(y)$$

Injects nonlinearity, enabling the network to approximate complex functions that simple linear models cannot.

Ex) ReLU: $\text{ReLU}(x) = \max(0,x)$

{% include end-box.html %}

## 2.Universal Approximation Theorem (UAT)
A neural network with a single hidden layer, provided it has a sufficient number of neurons (capacity), can approximate any continuous function to a desired level of accuracy. 


## 3.Deep learning
Essential components of Deep learning: **Neural network**, **Loss function** and **Optimizer**.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Softmax function**<br>
: Usually used for final layer of classification model. 

Each class's output value is transformed into a number  between 0 and 1.<br>
Total sum of all classes should be 1. 

{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Cross-entropy Loss**<br>
: To calculate the dissimilarity between the predicted probability distribution and the true probability distribution.

It is usually used in classification model. 

{% include end-box.html %}

{% include end-box.html %}

# W2-2
{% include start-box.html class="math-box"%}

## 1.Pytorch basics
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**1-1) Tensor**<br>
The core data structure used in PyTorch, taking the form of a multi-dimensional array.<br>
The inputs, outputs, and all parameters (like weights) of a deep learning model are represented in the form of Tensors. 

**Key attributes**
- `shape`: Describes the dimensions or shape of the Tensor (e.g., (3, 4) for a 3x4 matrix). 

- `dtype`: The data type of the elements stored in the Tensor (e.g., float32). 

- `device`: Indicates which hardware the Tensor is stored on (CPU or GPU/CUDA).

**Basic operations**
- Addition/Multiplication: Element-wise operations like `x + y` and `x * y` are supported. 

- Reshaping: You can change a Tensor's shape without altering its data using methods like `.view()`. 

- Matrix Multiplication: `torch.matmul(a, b)` performs matrix multiplication, which is a core operation in the layers of an MLP (Multi-layer Perceptron). 

```python
import torch

# Define the first matrix (mat1) with a shape of (2, 3)
mat1 = torch.tensor([
    [1, 2, 3],
    [4, 5, 6]
])

# Define the second matrix (mat2) with a shape of (3, 2)
# For matmul, the number of columns in mat1 must equal the number of rows in mat2.
mat2 = torch.tensor([
    [10, 20],
    [30, 40],
    [50, 60]
])

# Perform matrix multiplication
result = torch.matmul(mat1, mat2)

# --- Print the inputs and the final result ---

print("Input Matrix 1 (mat1):")
print(mat1)
print("\nShape:", mat1.shape)
print("-" * 30)

print("Input Matrix 2 (mat2):")
print(mat2)
print("\nShape:", mat2.shape)
print("-" * 30)

print("Result of torch.matmul(mat1, mat2):")
print(result)
print("\nShape:", result.shape)
```
```python
Input Matrix 1 (mat1):
tensor([[1, 2, 3],
        [4, 5, 6]])

Shape: torch.Size([2, 3])
------------------------------
Input Matrix 2 (mat2):
tensor([[10, 20],
        [30, 40],
        [50, 60]])

Shape: torch.Size([3, 2])
------------------------------
Result of torch.matmul(mat1, mat2):
tensor([[220, 280],
        [490, 640]])

Shape: torch.Size([2, 2])
```

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**1-2) Batch**<br>
A unit that groups multiple data points into a single Tensor to be processed all at once. For example, if a single 28x28 image has a shape of (28, 28), a batch of 4 such images would have a shape of (4, 28, 28). 

Q: Why Batch?
- Individual Processing: If you process 100 images one by one, the model must perform 100 separate matrix multiplication operations. This is highly inefficient as it fails to properly utilize the parallel processing power of a GPU. 

- Batch Processing: By grouping 100 images into a single batch tensor (e.g., [100, 784]), the model can process all 100 images with just one single, large matrix multiplication operation. 

In conclusion, batch processing is essential because it maximizes the parallel computing capabilities of modern hardware (especially GPUs) to significantly speed up the training of deep learning models.

{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**1-3) einsum**<br>
`einsum` is a powerful function that allows you to perform complex multiplications, summations, and dimension changes on multi-dimensional Tensors using a concise string-based notation. 

- How it works: Based on the Einstein summation convention, it defines an operation by specifying the dimensions of the input and output tensors in a string. For example, 'ij, jk -> ik' defines matrix multiplication: it takes a matrix of shape i x j and another of shape j x k, and produces an output matrix of shape i x k. 

- Usefulness:

    - Beyond simple matrix multiplication, it can intuitively express complex operations on tensors with many dimensions.

    - einsum can simply implement a more complex operation, such as summing over the channel dimension (c) of a batch of color images (b, c, h, w) to effectively make them grayscale ('bchw -> bhw'). 

```python
import torch

print("--- Example 1: Matrix Multiplication ('ij,jk->ik') ---")

# Define two matrices with simple integers
mat1 = torch.tensor([[1, 2, 3], [4, 5, 6]])      # Shape: (2, 3) -> Dimensions 'i, j'
mat2 = torch.tensor([[10, 20], [30, 40], [50, 60]]) # Shape: (3, 2) -> Dimensions 'j, k'

# Perform matrix multiplication using einsum.
# The notation 'ij,jk->ik' means:
# 1. Take a tensor with dimensions (i, j) and another with (j, k).
# 2. Multiply them and sum over the common dimension 'j'.
# 3. The resulting output tensor should have dimensions (i, k).
result_einsum = torch.einsum('ij,jk->ik', mat1, mat2)

# --- Print the inputs and the result ---
print("Input Matrix 1 (Shape: 2,3):")
print(mat1)

print("\nInput Matrix 2 (Shape: 3,2):")
print(mat2)

print("\nResult from einsum('ij,jk->ik'):")
print(result_einsum)
```
```python
--- Example 1: Matrix Multiplication ('ij,jk->ik') ---
Input Matrix 1 (Shape: 2,3):
tensor([[1, 2, 3],
        [4, 5, 6]])

Input Matrix 2 (Shape: 3,2):
tensor([[10, 20],
        [30, 40],
        [50, 60]])

Result from einsum('ij,jk->ik'):
tensor([[220, 280],
        [490, 640]])
```
<br>
```python
import torch

print("\n--- Example 2: Summing a Dimension ('bchw->bhw') ---")

# Create a batch of 2 "images". Each image has 3 channels (RGB) and is 2x2 pixels.
# Shape: (batch, channels, height, width) -> (2, 3, 2, 2)
image_batch = torch.tensor([
    # Image 1
    [[[1, 1], [1, 1]],  # Channel 0 (R)
     [[2, 2], [2, 2]],  # Channel 1 (G)
     [[3, 3], [3, 3]]], # Channel 2 (B)

    # Image 2
    [[[4, 4], [4, 4]],  # Channel 0 (R)
     [[5, 5], [5, 5]],  # Channel 1 (G)
     [[6, 6], [6, 6]]]  # Channel 2 (B)
])

# Sum over the channel dimension 'c'.
# The input is 'bchw' and the output is 'bhw', so the 'c' dimension is summed and removed.
result_sum = torch.einsum('bchw->bhw', image_batch)

# --- Print the inputs and the result ---
print("Input Batch of Images (Shape: 2,3,2,2):")
print(image_batch)

print("\nResult from einsum('bchw->bhw'):")
print(result_sum)

print("\nResult Shape:", result_sum.shape)
```
```python
--- Example 2: Summing a Dimension ('bchw->bhw') ---
Input Batch of Images (Shape: 2,3,2,2):
tensor([[[[1, 1],
          [1, 1]],

         [[2, 2],
          [2, 2]],

         [[3, 3],
          [3, 3]]],


        [[[4, 4],
          [4, 4]],

         [[5, 5],
          [5, 5]],

         [[6, 6],
          [6, 6]]]])

Result from einsum('bchw->bhw'):
tensor([[[ 6,  6],
         [ 6,  6]],

        [[15, 15],
         [15, 15]]])

Result Shape: torch.Size([2, 2, 2])
```
{% include end-box.html %}
{% include end-box.html %}

## 2.Autograd
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
Autograd is the automatic differentiation engine in PyTorch. 

- $\textbf{Model and Loss Function:}$
  - Linear Model: $\hat{\vec{y}} = \vec{x}\mathbf{W} + \vec{b}$
  - Loss Function (MSE): $\mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)^2$

- $\textbf{Manual Calculation (Analytic form):}$ Differentiating the loss function $\mathcal{L}$ with respect to the parameters $\mathbf{W}$ and $\vec{b}$ yields complex expressions:
  - $\frac{\partial\mathcal{L}}{\partial\mathbf{W}} = \frac{2}{N} \sum_{i=1}^{N} \vec{x}_i(\hat{y}_i - y_i)$
  - $\frac{\partial\mathcal{L}}{\partial\vec{b}} = \frac{2}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)$

```python
import torch

# 1. Create Sample Data
# We'll create some simple data points since they aren't defined in the PDF snippet.
# Let's assume the true relationship is y = 2x + 1
data = torch.tensor([[0.0], [1.0], [2.0], [3.0]])   # Input data (x)
target = torch.tensor([[1.0], [3.0], [5.0], [7.0]]) # True labels (y)

# 2. Define Model Parameters (from PDF page 35)
# We set 'requires_grad=True' to tell PyTorch to track gradients for these tensors.
W = torch.tensor([[1.0]], requires_grad=True)
b = torch.tensor([0.0], requires_grad=True)

# 3. Forward Pass: Calculate the model's prediction
# Linear model: prediction = xW + b
prediction = data @ W + b

# 4. Calculate the Loss (from PDF page 34)
# We use the Mean Squared Error (MSE) loss function.
loss = torch.mean((prediction - target)**2)

# 5. Backward Pass: Compute gradients using autograd
# This single command calculates the gradient of the loss
# with respect to all tensors that have requires_grad=True (i.e., W and b).
loss.backward()

# --- Print the results ---

print(f"The calculated loss is: {loss.item():.4f}\n")

# The computed gradients are now stored in the .grad attribute of the parameters.
print("Autograd Gradient of loss with respect to W:")
print(W.grad)

print("\nAutograd Gradient of loss with respect to b:")
print(b.grad)
```
```python
The calculated loss is: 4.5000

Autograd Gradient of loss with respect to W:
tensor([[-7.]])

Autograd Gradient of loss with respect to b:
tensor([-3.])
```
{% include end-box.html %}

## 3. Goal of Deep learning
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Goal of Deep Learning**<br>
The Ultimate Goal of Deep Learning: Generalization<br>
The ultimate goal of deep learning is not simply to master the training data. The true objective is to minimize the error on new, unseen test data. This capability is known as  generalization.

While the Universal Approximation Theorem states that a network with enough capacity can mimic any function, is a bigger, "deeper" network always better?<br> 
This question introduces the primary obstacle to achieving good generalization: **overfitting**.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Overfitting**<br>
Overfitting: The Main Obstacle to Generalization.<br>
Overfitting is what happens when a model learns the training data too well, to the point where it starts to memorize not only the underlying patterns but also the random noise within the data.

The goal of deep learning is not to achieve zero error on the training set.<br> 
It is to build a model that minimizes the test-time error by finding the right balance between fitting the data and avoiding overfitting, thus achieving good generalization.

Test-time error is the error a machine learning model produces when evaluated on new, unseen data that was not used during its training. The ultimate goal of deep learning is to minimize this error, as it measures how well the model can generalize to real-world scenarios.
{% include end-box.html %}

## 4. The Three Sources of Test-Time Error
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
The ultimate goal of a deep learning model is to minimize the test-time error on unseen data.  This error can be broken down into three primary sources: **Noise**/**Bias**/**Variance**

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Noise**<br>
This is the unavoidable, random error inherent in the data itself. It can come from sources like physical sensor noise or human errors during data labeling. This component of the error is irreducible.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Bias**<br>
Bias is the error that arises when a model is too simple (i.e., has low capacity) to capture the true underlying function or patterns in the data. A model with high bias is in a state of 
underfitting and cannot fit the training data well. For example, the error that occurs when trying to approximate a complex sine wave with only a few ReLU units is due to bias.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Variance**<br>
Variance is the error that arises from a model's sensitivity to small fluctuations in the training data. This typically happens when a model is too complex (i.e., has high capacity) and learns the noise in the training data instead of the true signal. A high-variance model will change significantly if the training data changes slightly, which leads to overfitting.
{% include end-box.html %}

{% include end-box.html %}


{% include end-box.html %}









<div class="indented-paragraph" markdown="1">

</div>

{% include start-box.html class="math-box-inner" font_size="0.8em"%}

{% include end-box.html %}


{% include start-box.html class="math-box"%}

{% include end-box.html %}