---
layout: single
title:  "ML total"
category: "Machine Learning"
tag: [Machine Learning, Artificial Intelligence]
header:
  teaser: #../images/2025-07-08-CT_6/image-20250708184503610.png
toc: true
toc_sticky: true
toc_label: CONTENTS
toc_icon: "fa-solid fa-seedling" # More icons: https://fontawesome.com/v6/search?ic=free
author_profile: false
sidebar:
    nav: "counts"
search: true # Change true to false if you don't want to have this article be searched 
redirect_from:
    - /Machine Learning/ML_1
use_math: true
---

**[Reference]** <br>
$\bullet$ [Machine Learning Q and AI](https://sebastianraschka.com/books/ml-q-and-ai/#table-of-contents)
{: .notice--success}

# W1-2
{% include start-box.html class="math-box"%}
## 1.Embeddings
- Embedding vectors or embeddings for short, encod relatively high-dimensional data into relatively low-dimensional vectors. 
    <div class="indented-paragraph" markdown="1">
    We can apply embedding methods to create a $\underbrace{\text{dense and continuous}}_{\text{Two characteristics of embedding}}$ vector.<br>
        - Dense: Embedding transform sparse vector into dense vector.<br>
        - Continuous: Similar data are embedded into similar vector by embedding.
    </div>

- Similar inputs are mapped to nearby locations (similarity is preserved).

- Mainly used for similarity and distance computations.

## 2.Latent space
The space into which embedding vectors are mapped (feature space).<br>
Preservation of similarity is not necessary condition for latent space.

## 3.Representation
A representation is an encoded, typically intermediate form of an input. <br>
For instance, an embedding vector or vector in the latent space is a representation of the input. <br>
However, simpler form of encoded vectors like one-hot encoded vectors also can be considered representation.

{% include end-box.html %}

# W2-1
{% include start-box.html class="math-box"%}

## 1.Neural Network
What we do in deep learning: Transform (refine) input features into better embedding.

Essential components of a Neural Network: **Linear transformation** and **Activation function**.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
1) Linear transformation

$$y=Wx+b$$

Projects the data into a different coordinate system, continually transforming the feature space.

{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
2) Activation function

$$\hat{y} = \sigma(y)$$

Injects nonlinearity, enabling the network to approximate complex functions that simple linear models cannot.

Ex) ReLU: $\text{ReLU}(x) = \max(0,x)$

{% include end-box.html %}

## 2.Universal Approximation Theorem (UAT)
A neural network with a single hidden layer, provided it has a sufficient number of neurons (capacity), can approximate any continuous function to a desired level of accuracy. 


## 3.Deep learning
Essential components of Deep learning: **Neural network**, **Loss function** and **Optimizer**.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Softmax function**<br>
: Usually used for final layer of classification model. 

Each class's output value is transformed into a number  between 0 and 1.<br>
Total sum of all classes should be 1. 

{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Cross-entropy Loss**<br>
: To calculate the dissimilarity between the predicted probability distribution and the true probability distribution.

It is usually used in classification model. 

{% include end-box.html %}

{% include end-box.html %}

# W2-2
{% include start-box.html class="math-box"%}

## 1.Pytorch basics
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**1-1) Tensor**<br>
The core data structure used in PyTorch, taking the form of a multi-dimensional array.<br>
The inputs, outputs, and all parameters (like weights) of a deep learning model are represented in the form of Tensors. 

**Key attributes**
- `shape`: Describes the dimensions or shape of the Tensor (e.g., (3, 4) for a 3x4 matrix). 

- `dtype`: The data type of the elements stored in the Tensor (e.g., float32). 

- `device`: Indicates which hardware the Tensor is stored on (CPU or GPU/CUDA).

**Basic operations**
- Addition/Multiplication: Element-wise operations like `x + y` and `x * y` are supported. 

- Reshaping: You can change a Tensor's shape without altering its data using methods like `.view()`. 

- Matrix Multiplication: `torch.matmul(a, b)` performs matrix multiplication, which is a core operation in the layers of an MLP (Multi-layer Perceptron). 

```python
import torch

# Define the first matrix (mat1) with a shape of (2, 3)
mat1 = torch.tensor([
    [1, 2, 3],
    [4, 5, 6]
])

# Define the second matrix (mat2) with a shape of (3, 2)
# For matmul, the number of columns in mat1 must equal the number of rows in mat2.
mat2 = torch.tensor([
    [10, 20],
    [30, 40],
    [50, 60]
])

# Perform matrix multiplication
result = torch.matmul(mat1, mat2)

# --- Print the inputs and the final result ---

print("Input Matrix 1 (mat1):")
print(mat1)
print("\nShape:", mat1.shape)
print("-" * 30)

print("Input Matrix 2 (mat2):")
print(mat2)
print("\nShape:", mat2.shape)
print("-" * 30)

print("Result of torch.matmul(mat1, mat2):")
print(result)
print("\nShape:", result.shape)
```
```python
Input Matrix 1 (mat1):
tensor([[1, 2, 3],
        [4, 5, 6]])

Shape: torch.Size([2, 3])
------------------------------
Input Matrix 2 (mat2):
tensor([[10, 20],
        [30, 40],
        [50, 60]])

Shape: torch.Size([3, 2])
------------------------------
Result of torch.matmul(mat1, mat2):
tensor([[220, 280],
        [490, 640]])

Shape: torch.Size([2, 2])
```

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**1-2) Batch**<br>
A unit that groups multiple data points into a single Tensor to be processed all at once. For example, if a single 28x28 image has a shape of (28, 28), a batch of 4 such images would have a shape of (4, 28, 28). 

Q: Why Batch?
- Individual Processing: If you process 100 images one by one, the model must perform 100 separate matrix multiplication operations. This is highly inefficient as it fails to properly utilize the parallel processing power of a GPU. 

- Batch Processing: By grouping 100 images into a single batch tensor (e.g., [100, 784]), the model can process all 100 images with just one single, large matrix multiplication operation. 

In conclusion, batch processing is essential because it maximizes the parallel computing capabilities of modern hardware (especially GPUs) to significantly speed up the training of deep learning models.

{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**1-3) einsum**<br>
`einsum` is a powerful function that allows you to perform complex multiplications, summations, and dimension changes on multi-dimensional Tensors using a concise string-based notation. 

- How it works: Based on the Einstein summation convention, it defines an operation by specifying the dimensions of the input and output tensors in a string. For example, 'ij, jk -> ik' defines matrix multiplication: it takes a matrix of shape i x j and another of shape j x k, and produces an output matrix of shape i x k. 

- Usefulness:

    - Beyond simple matrix multiplication, it can intuitively express complex operations on tensors with many dimensions.

    - einsum can simply implement a more complex operation, such as summing over the channel dimension (c) of a batch of color images (b, c, h, w) to effectively make them grayscale ('bchw -> bhw'). 

```python
import torch

print("--- Example 1: Matrix Multiplication ('ij,jk->ik') ---")

# Define two matrices with simple integers
mat1 = torch.tensor([[1, 2, 3], [4, 5, 6]])      # Shape: (2, 3) -> Dimensions 'i, j'
mat2 = torch.tensor([[10, 20], [30, 40], [50, 60]]) # Shape: (3, 2) -> Dimensions 'j, k'

# Perform matrix multiplication using einsum.
# The notation 'ij,jk->ik' means:
# 1. Take a tensor with dimensions (i, j) and another with (j, k).
# 2. Multiply them and sum over the common dimension 'j'.
# 3. The resulting output tensor should have dimensions (i, k).
result_einsum = torch.einsum('ij,jk->ik', mat1, mat2)

# --- Print the inputs and the result ---
print("Input Matrix 1 (Shape: 2,3):")
print(mat1)

print("\nInput Matrix 2 (Shape: 3,2):")
print(mat2)

print("\nResult from einsum('ij,jk->ik'):")
print(result_einsum)
```
```python
--- Example 1: Matrix Multiplication ('ij,jk->ik') ---
Input Matrix 1 (Shape: 2,3):
tensor([[1, 2, 3],
        [4, 5, 6]])

Input Matrix 2 (Shape: 3,2):
tensor([[10, 20],
        [30, 40],
        [50, 60]])

Result from einsum('ij,jk->ik'):
tensor([[220, 280],
        [490, 640]])
```
<br>
```python
import torch

print("\n--- Example 2: Summing a Dimension ('bchw->bhw') ---")

# Create a batch of 2 "images". Each image has 3 channels (RGB) and is 2x2 pixels.
# Shape: (batch, channels, height, width) -> (2, 3, 2, 2)
image_batch = torch.tensor([
    # Image 1
    [[[1, 1], [1, 1]],  # Channel 0 (R)
     [[2, 2], [2, 2]],  # Channel 1 (G)
     [[3, 3], [3, 3]]], # Channel 2 (B)

    # Image 2
    [[[4, 4], [4, 4]],  # Channel 0 (R)
     [[5, 5], [5, 5]],  # Channel 1 (G)
     [[6, 6], [6, 6]]]  # Channel 2 (B)
])

# Sum over the channel dimension 'c'.
# The input is 'bchw' and the output is 'bhw', so the 'c' dimension is summed and removed.
result_sum = torch.einsum('bchw->bhw', image_batch)

# --- Print the inputs and the result ---
print("Input Batch of Images (Shape: 2,3,2,2):")
print(image_batch)

print("\nResult from einsum('bchw->bhw'):")
print(result_sum)

print("\nResult Shape:", result_sum.shape)
```
```python
--- Example 2: Summing a Dimension ('bchw->bhw') ---
Input Batch of Images (Shape: 2,3,2,2):
tensor([[[[1, 1],
          [1, 1]],

         [[2, 2],
          [2, 2]],

         [[3, 3],
          [3, 3]]],


        [[[4, 4],
          [4, 4]],

         [[5, 5],
          [5, 5]],

         [[6, 6],
          [6, 6]]]])

Result from einsum('bchw->bhw'):
tensor([[[ 6,  6],
         [ 6,  6]],

        [[15, 15],
         [15, 15]]])

Result Shape: torch.Size([2, 2, 2])
```
{% include end-box.html %}
{% include end-box.html %}

## 2.Autograd
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
Autograd is the automatic differentiation engine in PyTorch. 

- $\textbf{Model and Loss Function:}$
  - Linear Model: $\hat{\vec{y}} = \vec{x}\mathbf{W} + \vec{b}$
  - Loss Function (MSE): $\mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)^2$

- $\textbf{Manual Calculation (Analytic form):}$ Differentiating the loss function $\mathcal{L}$ with respect to the parameters $\mathbf{W}$ and $\vec{b}$ yields complex expressions:
  - $\frac{\partial\mathcal{L}}{\partial\mathbf{W}} = \frac{2}{N} \sum_{i=1}^{N} \vec{x}_i(\hat{y}_i - y_i)$
  - $\frac{\partial\mathcal{L}}{\partial\vec{b}} = \frac{2}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)$

```python
import torch

# 1. Create Sample Data
# We'll create some simple data points since they aren't defined in the PDF snippet.
# Let's assume the true relationship is y = 2x + 1
data = torch.tensor([[0.0], [1.0], [2.0], [3.0]])   # Input data (x)
target = torch.tensor([[1.0], [3.0], [5.0], [7.0]]) # True labels (y)

# 2. Define Model Parameters (from PDF page 35)
# We set 'requires_grad=True' to tell PyTorch to track gradients for these tensors.
W = torch.tensor([[1.0]], requires_grad=True)
b = torch.tensor([0.0], requires_grad=True)

# 3. Forward Pass: Calculate the model's prediction
# Linear model: prediction = xW + b
prediction = data @ W + b

# 4. Calculate the Loss (from PDF page 34)
# We use the Mean Squared Error (MSE) loss function.
loss = torch.mean((prediction - target)**2)

# 5. Backward Pass: Compute gradients using autograd
# This single command calculates the gradient of the loss
# with respect to all tensors that have requires_grad=True (i.e., W and b).
loss.backward()

# --- Print the results ---

print(f"The calculated loss is: {loss.item():.4f}\n")

# The computed gradients are now stored in the .grad attribute of the parameters.
print("Autograd Gradient of loss with respect to W:")
print(W.grad)

print("\nAutograd Gradient of loss with respect to b:")
print(b.grad)
```
```python
The calculated loss is: 4.5000

Autograd Gradient of loss with respect to W:
tensor([[-7.]])

Autograd Gradient of loss with respect to b:
tensor([-3.])
```
{% include end-box.html %}

## 3. Goal of Deep learning
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Goal of Deep Learning**<br>
The Ultimate Goal of Deep Learning: Generalization<br>
The ultimate goal of deep learning is not simply to master the training data. The true objective is to minimize the error on new, unseen test data. This capability is known as  generalization.

While the Universal Approximation Theorem states that a network with enough capacity can mimic any function, is a bigger, "deeper" network always better?<br> 
This question introduces the primary obstacle to achieving good generalization: **overfitting**.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Overfitting**<br>
Overfitting: The Main Obstacle to Generalization.<br>
Overfitting is what happens when a model learns the training data too well, to the point where it starts to memorize not only the underlying patterns but also the random noise within the data.

The goal of deep learning is not to achieve zero error on the training set.<br> 
It is to build a model that minimizes the test-time error by finding the right balance between fitting the data and avoiding overfitting, thus achieving good generalization.

Test-time error is the error a machine learning model produces when evaluated on new, unseen data that was not used during its training. The ultimate goal of deep learning is to minimize this error, as it measures how well the model can generalize to real-world scenarios.
{% include end-box.html %}

## 4. The Three Sources of Test-Time Error
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
The ultimate goal of a deep learning model is to minimize the test-time error on unseen data.  This error can be broken down into three primary sources: **Noise**/**Bias**/**Variance**

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Noise**<br>
This is the unavoidable, random error inherent in the data itself. It can come from sources like physical sensor noise or human errors during data labeling. This component of the error is irreducible.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Bias**<br>
Bias is the error that arises when a model is too simple (i.e., has low capacity) to capture the true underlying function or patterns in the data. A model with high bias is in a state of 
underfitting and cannot fit the training data well. For example, the error that occurs when trying to approximate a complex sine wave with only a few ReLU units is due to bias.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Variance**<br>
Variance is the error that arises from a model's sensitivity to small fluctuations in the training data. This typically happens when a model is too complex (i.e., has high capacity) and learns the noise in the training data instead of the true signal. A high-variance model will change significantly if the training data changes slightly, which leads to overfitting.
{% include end-box.html %}

For a given dataset, the error comes from 

$$\mathbb{E}_{\mathcal{D}}[\mathbb{E}_y[L[x]]] = \mathbb{E}_{\mathcal{D}}[\underbrace{(f[x, \phi[\mathcal{D}]] - f_{\mu}[x])^2}_{\text{variance}}] + \underbrace{(f_{\mu}[x] - \mu[x])^2}_{\text{bias}} + \underbrace{\sigma^2}_{\text{noise}}.$$

Thus, reducing error means reducing variance and bias.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Bias-Variance Trade-off**<br>
The test error can be mathematically expressed as the sum of variance, squared bias, and noise.<br>
Since noise is irreducible, minimizing the total error requires minimizing both bias and variance.<br> 

However, these two sources of error are often in opposition to each other:

- As you increase the model's capacity (e.g., by using a deeper network or more neurons), the model can learn more complex functions, which causes the bias to decrease. 

- But, for a fixed amount of training data, increasing the model's capacity also makes it more likely to fit the noise in that specific dataset, which causes the variance to increase. 

This inverse relationship, where decreasing one error source tends to increase the other, is known as the **Bias-Variance Trade-off**.<br>

Therefore, our goal in training a model is to find a "**sweet spot**"—a point of optimal model capacity that minimizes the combined sum of bias and variance.<br> 
A model that is too simple will have high bias, and a model that is too complex will have high variance.<br>
The key is to build a model that is just right to achieve the best generalization performance.
{% include end-box.html %}
{% include end-box.html %}

## 5.Double descent
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Classical vs. Modern Perspectives**<br>
- Classical Machine Learning: Previously, we learned that as a model's complexity increases past a certain point, the test error continuously increases due to overfitting, following a U-shaped curve. This is the traditional viewpoint of the Bias-Variance Trade-off. 

- Modern Machine Learning: However, a different phenomenon is observed in modern deep learning models, which are often over-parameterized—meaning they have far more parameters than there are data points.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**What is the Double Descent Phenomenon?**<br>
The Double Descent phenomenon describes how, as a model's complexity continues to increase, the test error, after reaching a peak, starts to decrease again instead of continuously increasing as classical theory would suggest. 

1.Under-fitting Regime: When model complexity is low, both training and test error decrease. 

2.Classical Overfitting Regime: As the model's complexity approaches the point where it can memorize the training data, the test error begins to rise. 

3.Interpolation Point: This is the point where the model perfectly memorizes the training data. The test error reaches its peak here. 

4.Modern Overfitting / Double Descent Regime: Surprisingly, as the model's complexity (number of parameters) increases far beyond this threshold, the test error begins to decrease again. 
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Double Descent in Practice (Empirical Evidence)**<br>
The double descent phenomenon challenges the classical wisdom that making a model too complex will inevitably lead to worse performance due to overfitting.<br> 
Instead, it suggests that in modern deep learning, making models extremely large and entering the over-parameterized regime can actually lead to better generalization and lower test error.<br> 
This is a key theory that helps explain why the massive models used today, such as Large Language Models (LLMs), can perform so well. 
{% include end-box.html %}
{% include end-box.html %}

# W3-1
{% include start-box.html class="math-box"%}

## 1.Bias-Variance Trade-off
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**The Root of the Problem: Over-reliance on Data**<br>
The lecture material identifies the data itself as the fundamental cause of overfitting. <br>
This is because the data we use is always limited in quantity and is merely a sample representing a partial observation of the real world. <br>
Overfitting occurs when a model depends too heavily on this imperfect data, memorizing its random noise instead of learning its true underlying patterns.<br>

Therefore, the question of how to suppress overfitting leads to the question: "How can we reduce the influence that the data has on the model?".
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**The Philosophical Solution: The Bayesian Perspective**<br>
Core Philosophy: A good model should not rely solely on the observed data (the Likelihood).<br>
Instead, it must also incorporate a reasonable predefined belief (the Prior) that we hold before even seeing the data.<br>

Key Terms:
- Observation (Likelihood): The data that we actually observe and measure.

- Prior Belief: The assumptions or beliefs we have about the model before observing any data.

- Posterior: The updated, more refined belief that is formed by combining the observation (Likelihood) and the prior belief (Prior).

**The Coin Toss Example**<br>
The coin toss example in the lecture slides provides a simple illustration of this concept.

- Prior Belief: "I believe this coin is fair".

- Observation (Likelihood): "When I tossed the coin 10 times, 6 of them came up heads".

- Posterior Belief: "Based on the results, the coin might be slightly biased toward heads".

As this example shows, the Bayesian perspective is a rational method of reasoning that continuously updates our existing beliefs using new data, rather than blindly trusting the data alone.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Bayes' Rule: The Mathematical Formulation of the Philosophy**<br>

This Bayesian philosophy is formalized by the mathematical equation known as Bayes' Rule.<br>

$$p(\theta \vert \mathcal{D}) = \frac{p(\mathcal{D} \vert \theta)p(\theta)}{p(\mathcal{D})}$$

- $p(\theta \vert D)$ (Posterior): This is what we want to find. It's the probability of our model parameters θ being correct, given the data D.

- $p(D \vert \theta)$ (Likelihood): The influence of the data. It's the probability of observing our data D, given our current model parameters θ.

- $p(\theta)$ (Prior): The influence of our prior belief. It's our initial belief in the probability of the model parameters 

$\theta$ being correct, before seeing any data.

In conclusion, the Bayesian perspective provides a philosophical and mathematical foundation for preventing a deep learning model from overfitting to the training data ($p(D \vert \theta)$).<br> 
It does this by introducing a reasonable constraint based on what the parameters should look like ($p(\theta)$).<br> 
This is the core idea that leads directly to the next step: Regularization.
{% include end-box.html %}

## 2.MAP estimation and L2 regularization
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**MAP (Maximum a Posteriori) Estimation**<br>
Training a deep learning model is the process of finding the most plausible model parameters ($\theta$) for a given set of data (D).<br>
From a Bayesian perspective, this goal is defined as maximizing the posterior probability, a method known as MAP (Maximum a Posteriori) estimation. <br>

Mathematical Expression:

$$\theta_{\text{MAP}} = \arg\max_{\theta} p(\theta | \mathcal{D})$$

According to Bayes' Rule, this is equivalent to maximizing the product of the Likelihood and the Prior.<br> 

Transformed Equation:

$$\theta_{\text{MAP}} = \arg\max_{\theta} \left( p(\mathcal{D}|\theta)p(\theta) \right)$$

Because multiplication is computationally complex to differentiate, we apply a logarithm, which converts the product into a sum without changing the location of the maximum value.<br>

Log-Transformed Equation:

$$\theta_{\text{MAP}} = \arg\max_{\theta} \left( \log p(\mathcal{D}|\theta) + \log p(\theta) \right)$$

Our goal is now to find the parameters $\theta$ that maximize the sum of the log-likelihood (the probability of the data) and the log-prior (the probability of our belief).
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**The Core Assumption: A Prior Belief that "Good Weights are Small"**<br>
This is where the key idea to prevent overfitting is introduced.<br> 
We establish a prior belief that "the parameters ($\theta$) of a good model should be small and close to zero."<br>

Mathematical Assumption: This belief is mathematically formalized by assuming that the parameters $\theta$ follow a zero-mean Gaussian (normal) distribution.<br>  

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Why zero mean prior assumption?**<br>
The reason for the zero-mean prior assumption is to create a robust model whose output is not overly sensitive to small changes in the input data, which is achieved by keeping the magnitude of the model's parameters (weights) small.<br> 

Detailed Explanation Definition of a "Good Model": a good model is "robust to input data distribution's disturbance".<br> 
This means a stable model whose output does not significantly change even when there is slight noise or variation in the input data.<br>   

The Connection to L2 Regularization: Mathematically applying the prior belief that "parameters follow a zero-mean Gaussian distribution" has the same effect as adding an L2 Regularization term ($\lambda \Vert \theta \Vert^2$) to the objective function.<br> 
This penalty term forces the model to keep the magnitude of its parameters ($\Vert \theta \Vert^2$, the squared norm) as small as possible during training.<br>   
In conclusion, the zero-mean prior assumption is the mathematical implementation of the belief that "a good model should have parameters with small magnitudes."<br> 
It guides the model to find a stable and robust solution that is not overly sensitive to small variations in the input data.
{% include end-box.html %}

The log of this distribution's probability density function takes the following form:

$$\log p(\theta) \propto - \frac{\lambda}{2} \Vert\theta\Vert^2$$

Data Term (Likelihood) Assumption: Similarly, we assume that the error between the model's prediction and the true value also follows a zero-mean Gaussian distribution.<br>  
Taking the log of this results in a form similar to the commonly used Mean Squared Error (MSE).

$$\log p(\mathcal{D}|\theta) \propto - \Vert \vec{y} - f_{\theta}(\vec{x}) \Vert^2$$

{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}


**The Birth of L2 Regularization: Connecting Theory and Practice**<br>
By substituting the two assumptions from the previous step into the log-transformed MAP equation and then converting the maximization problem into a minimization problem (by flipping the sign), we arrive at the final equation. 

Final Derived Equation:

$$\theta_{\text{MAP}} = \arg\min_{\theta} \left( \Vert \vec{y} - f_{\theta}(\vec{x}) \Vert^2 + \lambda\Vert\theta\Vert^2 \right)$$

The meaning of this final equation is as follows:

$\Vert y-f_{\theta}(x) \Vert ^2$: This is the **Loss Function**, representing the error between the model's prediction and the true value. 

$\lambda \Vert \theta \Vert ^2$: This is the **L2 Regularization term**, which acts as a penalty proportional to the squared magnitude of the model's parameters (weights). 


In conclusion, by mathematically applying a reasonable prior belief ("weights should be small") from a Bayesian perspective, we naturally derive the L2 Regularization technique used in practice to prevent overfitting.<br> 
This shows that using L2 regularization is not just a technical trick, but a practical implementation of the Bayesian philosophy of finding an optimal solution that considers both the data and a prior belief.
{% include end-box.html %}

## 3.Effect of regularization 
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Beyond the Loss Function to the Objective Function**<br> 
When we train a deep learning model, we are not just minimizing the error on the data (the loss).<br> 
When applying regularization, we are actually minimizing an Objective Function. <br>
This is defined as: 

**Objective Function = Residual Function + Regularization Function** 

**Residual Function**: This is the same as the traditional Loss Function.<br> 
<div class="indented-paragraph" markdown="1">
It measures the error between the model's predictions and the actual data. 
</div>
**Regularization Function**: This is the term that adds a penalty to prevent the model's parameters (weights) from becoming too large. 
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**The Effect of Regularization Strength ($\lambda$)**<br>
$\lambda$ is a hyperparameter that controls the strength of the regularization.<br>
The model's learned outcome varies significantly depending on the value of $\lambda$.<br> 

- $\lambda = 0$ (No Regularization): The model only tries to reduce the data error (the residual function).<br> 
As a result, it learns the noise in the training data, leading to a state of overfitting with a highly complex and fluctuating prediction curve.<br> 

- When $\lambda$ is appropriate (e.g., 0.001): The model must reduce the data error while also keeping the magnitude of its weights small.<br> 
This results in a smoother, more generalized prediction curve that is less affected by noise.<br> 

- When $\lambda$ is too large (e.g., 0.1): The penalty for large weights becomes too strong, preventing the model from learning the underlying pattern in the data.<br> 
This leads to a state of underfitting, where the prediction is overly simplistic. 
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**The Changing Loss Landscape**<br>
Regularization also has a positive effect on the optimization process.<br> 

- Before Regularization (Loss): The original loss surface can be very complex, rugged, and contain many local minima.<br> 
An optimizer, like gradient descent, is at risk of getting trapped in one of these local minima and failing to find the global optimum. <br>

- After Regularization (Loss + Regularization): When the regularization term (which has a simple, bowl-like shape) is added, the surface of the total objective function becomes much smoother, and the number of local minima is reduced.<br> 
On this gentler landscape, the optimizer can more reliably and stably find a good solution.
{% include end-box.html %}
{% include end-box.html %}

# W3-2
{% include start-box.html class="math-box"%}

## 1.Gradient Descent
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**The Goal of Model Training: Minimizing the Objective Function**<br>
Training a model is ultimately a process of minimizing the value of an Objective Function, which represents the model's performance.<br> 
This objective function is generally composed of the sum of a 'Residual' term, which is the difference between the predicted and actual values, and a 'Regularization' term, which prevents the model from overfitting.

This objective function can be considered as a 'Loss surface', which looks like a parabolic graph (e.g., $y=x^2$).<br> 
Our goal is to find the lowest point on this surface, the very bottom of the valley.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**The Principle of Gradient Descent**<br>
Gradient Descent is the most fundamental method for finding the minimum point of the loss surface.<br> 
This method consists of two key components:

- Direction: In which direction from the current position does the loss value decrease the fastest? 

- Amount: How large of a step should be taken in that direction? 

These two factors are expressed in the core update formula for Gradient Descent:

$$\theta \leftarrow \theta - \eta \nabla f(\theta)$$

- $\nabla f(\theta)$(The Gradient): This determines the 'direction'. The gradient indicates the direction of the steepest ascent in the function's value from the current position ($\theta$). Therefore, we must move in the opposite direction (indicated by the - sign) to reduce the loss.

- $\eta$ (The Learning Rate): This determines the 'amount', or the size of the step to be taken.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Controlling the Step Size: The Importance of the Learning Rate**<br>
The **learning rate** ($\eta$) is a crucial hyperparameter that can determine the success or failure of model training.

- If the learning rate is too small (e.g., lr=0.05): The model moves towards the minimum very slowly and incrementally, resulting in long training times.

- If the learning rate is too large (e.g., lr=0.8): The step size is too big, which can cause it to completely miss the minimum and jump to the other side, a phenomenon known as **overshooting**. In this case, the process might continue to oscillate around the minimum or, in worse cases, diverge as the loss value increases.

The optimal learning rate is often found **empirically**.<br> 
A generally effective strategy is to start with a large learning rate and gradually decrease it as training progresses.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Automating Step Size Control: The Learning Rate Scheduler**<br>
Since manually adjusting the learning rate can be cumbersome, a **Learning Rate Scheduler** is used to automatically adjust the learning rate during the training process.<br> 
The lecture materials introduce several schedulers available in PyTorch:

- **StepLR**: Decreases the learning rate by a set ratio (gamma) at a specific interval (step_size).

- **Exponential Decay**: The learning rate decreases exponentially at each step.

- **Cosine Annealing**: The learning rate follows the shape of a cosine curve, decreasing smoothly and sometimes slightly increasing again.
{% include end-box.html %}

## 2.Backpropagation
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Three Methods for Calculating Gradients**<br>

- **Analytic Gradient**

- **Numerical Differentiation**

- **Automatic Differentiation**: This is the standard method used in deep learning.  Its core algorithm is backpropagation.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**The Core Principle of Backpropagation: The Chain Rule**<br>
A neural network is essentially one giant composite function, equivalent to a "**chain of simple functions**" nested one after another. 

$$y = f_4(f_3(f_2(f_1(x))))$$

Therefore, no matter how complex the neural network is, we can calculate its overall gradient by differentiating each component and then multiplying these derivatives together according to the Chain Rule. 
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Visualizing the Process: The Computational Graph**

<figure style="display: flex; flex-direction: column; align-items: center; margin-top: 0.5em; margin-bottom: 0.5em;">
  <img src="../images/2025-10-04-ML_total/Fig_1.png" alt="" 
       style="width: 55%; height: auto;">
   <figcaption style="font-size: 20px; margin-top: -0.5em;">
   The Computational Graph
   </figcaption>
</figure> 

- **The Forward Pass**
This is the process where the input $x$ and parameters $W$ and $b$ pass sequentially through each operation in the network (multiplication, addition, activation function, etc.) to compute the final loss value, $L$.  This is like a river flowing from upstream to downstream.

- **The Backward Pass**
This process works in the opposite direction of the forward pass. It starts from the final output, the loss $L$, and calculates the gradients in reverse using the **Chain Rule**. 
Let's break down the calculation process:

  - **Step 1**: The final output is the loss $L$. The derivative of $L$ with respect to $L$, $\frac{\partial L}{\partial L}$, is naturally 1. This is the starting point of backpropagation.
  - **Step 2**: The loss $L$ is defined as $(a−y)^2$, so the derivative of $L$ with respect to $a$ is $\frac{\partial L}{\partial a}=2(a−y)$. 
  - **Step 3**: To find the gradient with respect to $z$, $\frac{\partial L}{\partial z}$, we use the chain rule:

  $$\frac{\partial \mathcal{L}}{\partial z} = \frac{\partial \mathcal{L}}{\partial a} \cdot \frac{\partial a}{\partial z} = 2(a-y) \cdot \sigma'(z)$$

  We simply multiply the gradient from the previous step, $\frac{\partial L}{\partial a}$, by the local derivative of the current node ($\sigma '(z)$). 
  - **Step 4**: Now we calculate the gradients for the parameters we need to update, $W$ and $b$. This also uses the chain rule, leveraging the gradient $\frac{\partial L}{\partial z}$calculated in the previous step:
    - **Gradient of W**: $\frac{\partial\mathcal{L}}{\partial\mathbf{W}} = \frac{\partial\mathcal{L}}{\partial z} \cdot \frac{\partial z}{\partial\mathbf{W}} = \frac{\partial\mathcal{L}}{\partial z} \cdot \vec{x}$
    - **Gradient of b**: $\frac{\partial\mathcal{L}}{\partial\vec{b}} = \frac{\partial\mathcal{L}}{\partial z} \cdot \frac{\partial z}{\partial\vec{b}} = \frac{\partial\mathcal{L}}{\partial z} \cdot 1$

In this way, backpropagation efficiently computes the gradients for all parameters by moving from the output layer to the input layer, reusing the gradient calculated in the preceding step. Because this process happens sequentially and reuses results, it can be implemented simply and recursively, which is why it's called Automatic Differentiation. 
{% include end-box.html %}

## 3.Adam optimizer
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**The Limitations of Standard Gradient Descent**<br>
While standard Gradient Descent ($\theta \leftarrow \theta − \eta \nabla(\theta)$) is a good method for finding the minimum of an objective function, it isn't always efficient.<br> 
This method can be slow or struggle to find the optimal path on complex loss surfaces because it only considers the gradient at the current position and moves with a fixed learning rate ($\eta$).

The Adam (Adaptive Moment Estimation) optimizer was introduced to overcome these limitations. 
Adam is now one of the most widely used optimization algorithms in deep learning. 
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Adam's Two Key Improvements**<br>
Adam significantly enhances training efficiency by extending the concepts of '**Direction**' and '**Amount**' from standard Gradient Descent.

**1. Momentum: A Smarter Direction**
- **Standard Method**: Moves only in the direction of the gradient at the current position. 
- **Adam's Improvement**: Considers a moving average of past gradients. This is like a ball rolling down a hill, gaining inertia (momentum) that helps it to continue moving in a consistent direction with more speed. This allows for faster and more stable convergence, especially in situations where the gradient direction changes frequently. 

**2. Adaptive Scaling: A Smarter Step Size**
- Standard Method: Applies the same fixed learning rate (η) to all parameters. 
- Adam's Improvement: Adaptively adjusts the learning rate for each parameter individually.  This is achieved by tracking the variance of each parameter's past gradients.
    - If a parameter's gradient has been consistently large (high variance), it means it has already moved a lot, so its learning rate is reduced to make finer adjustments.
    - Conversely, if a parameter's gradient has been small (low variance), the learning rate is increased to accelerate its learning.

The table from the lecture material clearly illustrates the difference between standard Gradient Descent (GD) and Adam.

| Method | Direction term | Scaling term (Step Size) |
|:---|:---|:---|
| **GD** | $\nabla f(\theta)$ (current gradient) | $\eta$ (fixed) |
| **Adam** | $\hat{m}_t$ (momentum) | $\frac{\eta}{\sqrt{\hat{v}_t} + \epsilon}$ (adaptive) |

Here, $$\hat{m}_t$$ is the moving average of the 1st moment (the mean) of the gradients, and $\hat{v}_t$ is the moving average of the 2nd moment (the uncentered variance).<br>

Adaptive optimizers like Adam can be seen converging to the optimal point (the red star) much more quickly and reliably than standard stochastic gradient descent (sgd).
{% include end-box.html %}

## 4.The Problem with Deep Neural Networks and its Solution: Residual Connections
{% include start-box.html class="math-box"%}
**The Problem: Are Deeper Networks Always Better?**<br>
Theoretically, stacking more layers in a neural network should allow it to learn more complex patterns and thus improve its performance. However, in practice, after a certain depth, stacking more layers leads to a **degradation problem**, where the network's performance gets worse.

<figure style="display: flex; flex-direction: column; align-items: center; margin-top: 0.5em; margin-bottom: 0.5em;">
  <img src="../images/2025-10-04-ML_total/Fig_2.png" alt="" 
       style="width: 55%; height: auto;">
   <figcaption style="font-size: 20px; margin-top: -0.5em;">
   </figcaption>
</figure> 

The graph on page 50 of the lecture material clearly illustrates this issue. When trained on the CIFAR-10 dataset, a deeper 56-layer network shows higher training error and test error than a shallower 20-layer network.

The key takeaway here is that this is **not overfitting**. If it were overfitting, the training error would be very low while the test error would be high. Instead, this result indicates that the training process itself is failing for the deeper network. 

**The Cause**: **The Vanishing Gradient Problem**
The fundamental reason for the training failure in very deep networks is the Vanishing Gradient Problem.<br>
During backpropagation, the gradient is passed from the output layer to the input layer by continuously multiplying the Jacobian matrices of each layer according to the Chain Rule.

$$\frac{\partial\mathcal{L}}{\partial\vec{x}} = \left( \prod_{k=1}^{L} \mathbf{J}_k^\top \right) \frac{\partial\mathcal{L}}{\partial\vec{h}^{(L)}}, \text{where } J^{\top}_{k}=\frac{\partial h^{(k)}}{\partial h^{(k-1)}}$$

The problem is that the values in these matrices (e.g., the derivatives of activation functions or the weights themselves) are often less than 1.<br> 
When these small numbers are multiplied together dozens or hundreds of times, the gradient value shrinks exponentially, eventually becoming close to zero.<br> 
As a result, the earlier layers (those closer to the input) receive almost no gradient signal, their parameters are not updated, and learning effectively stops.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**The Solution: Residual Learning and Residual Connections**<br>
To solve this vanishing gradient problem, Kaiming He et al. proposed an innovative method in 2016 called **Residual Learning**.

- **The Core Idea**: Instead of having a network block learn a complex transformation directly, the structure is changed to have the block learn only the **difference (the residual)** from the input.

- **The Implementation**: This is achieved using a structure called a **Residual Connection** or **Skip Connection**. This connection takes the input to a block ($x$) and adds it directly to the block's output ($F(x)$) before passing it to the next layer.
  - **Standard Block**: $F(x)$
  - **Residual Block**: $F(x) + x$

<figure style="display: flex; flex-direction: column; align-items: center; margin-top: 0.5em; margin-bottom: 0.5em;">
  <img src="../images/2025-10-04-ML_total/Fig_3.png" alt="" 
       style="width: 100%; height: auto;">
   <figcaption style="font-size: 20px; margin-top: -0.5em;">
   </figcaption>
</figure> 

The diagrams visually contrast these structures. While a standard network has a sequential data flow ($f_1 \rightarrow f_2 \rightarrow f_3 \rightarrow f_4$), a residual network (ResNet) has "shortcuts" that bypass a block and add the input to the output. 
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**The Effect of Residual Connections: A Gradient Highway**<br>
This simple $+x$ operation solves the vanishing gradient problem for the following reason:<br>

During backpropagation, the gradient can flow back through this skip connection without being diminished.<br> 
In essence, it creates a "**highway**" for the gradient to travel back to earlier layers.<br> 
Because the derivative of the $+x$ identity path is 1, it ensures that a non-zero gradient can always be propagated, preventing it from vanishing even in very deep networks.<br> 
This allows for stable training of much deeper architectures.
{% include end-box.html %}
{% include end-box.html %}

# W4-1
{% include start-box.html class="math-box"%}

## 1.What is Computer Vision?
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Computer vision**<br>
Computer Vision is a field of artificial intelligence that enables machines to "see" and understand the visual world.

**1. Image Recognition & Understanding**
- Classification
- Detection
- Segmentation
- Caption Generation

**2. Image Generation & Transformation**
- Synthesis
- Inpainting
- Style Transfer
- Super-resolution

**3. 3D Spatial Understanding**
- Depth Prediction & Scene Reconstruction
{% include end-box.html %}

## 2.The Dawn of the Deep Learning Revolution: ImageNet and AlexNet
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**The Stagnation of Computer Vision and the Emergence of ImageNet**<br>
The ImageNet Challenge brought a revolutionary change in computer vision.

**What is ImageNet?:**<br> 
ImageNet is a massive dataset consisting of millions of high-resolution images (specifically 14,197,122) labeled with over a thousand categories (specifically 21,841).  Beyond its sheer scale, the dataset captured the complexity of the real world, with objects shown from various angles, under different lighting conditions, against cluttered backgrounds, and with occlusions, making it an extremely difficult challenge for the algorithms of the time . The ImageNet Challenge was a competition to see which model could achieve the best image classification performance on this dataset.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**The Dawn of the Deep Learning Era: The Shocking Debut of AlexNet (2012)**<br>
In 2012, a pivotal event occurred at the ImageNet Challenge that would change the course of computer vision history. A deep learning model called AlexNet, developed by Professor Geoffrey Hinton and his students, won the competition with an astonishingly low error rate of 16%. This was a massive leap forward compared to the second-place model, which had an error rate in the 26% range, and it served as a powerful demonstration of deep learning's potential to the entire world.

This event marked a complete paradigm shift in computer vision research from traditional methods to deep learning. In the years that followed, even deeper and more sophisticated models like VGG, GoogLeNet, and ResNet emerged, drastically reducing the error rate year after year. By 2015, a model finally surpassed the human error rate (approximately 5%).
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**The Keys to AlexNet's Success**<br>
How was AlexNet able to win by such a large margin?

**1. Revival of Deep Learning with GPUs**:<br> 
The concept of deep learning had existed for a long time, but it was not considered practical due to the immense computational power required to train networks with so many parameters. The AlexNet team solved this computational bottleneck by using GPUs (Graphics Processing Units), which are optimized for parallel processing. This allowed them to successfully train a deep neural network that was previously thought to be infeasible.

**2. Rediscovery of Convolutional Neural Networks (CNNs)**:<br> 
AlexNet adopted the Convolutional Neural Network (CNN) architecture, which is exceptionally effective for image processing. A CNN is a model specialized for extracting features while preserving the spatial structure of an image. The power of the CNN architecture was brought back into the spotlight through AlexNet's success.
{% include end-box.html %}

## 3.How CNNs Work: The Core Components
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**1. Convolution: The Task of Extracting Image Features**<br>
The most critical operation in a CNN is **convolution**. This is the process of detecting visual features such as lines, edges, textures, and color patterns in an image.

- **How it Works**:<br> 
A small matrix called a **Kernel** or **Filter** (e.g., 3x3) slides sequentially across the input image. At each position, the kernel performs an element-wise multiplication with the corresponding region of the image, and the results are summed up to produce a single value. By repeating this process across the entire image, a new matrix called a **Feature Map** is generated. 

- **The Role of the Kernel**:<br> 
The values within the kernel determine what kind of feature it can detect. For example, the kernel [[-1, -1, -1], [0, 0, 0], [1, 1, 1]] shown on page 41 of the lecture material is designed to detect horizontal edges.  In deep learning, the values for these kernels are learned automatically during the training process.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**2. Channels: The Depth of Color and Features**<br>
- **Input Channels (in_channels)**: 
This refers to the depth of the input data. For instance, a grayscale image has 1 channel, while a color (RGB) image has 3 channels for Red, Green, and Blue. When applying a convolution to a 3-channel image, a single **Kernel Set** consisting of three separate kernels (one for each channel) is used. The results from each channel's operation are then summed to produce a **single output channel**.

- **Output Channels (out_channels)**:<br> 
This is determined by the number of Kernel Sets used. If you use two different kernel sets (i.e., you want to extract two different types of features simultaneously), two output channels (feature maps) will be generated. In the PyTorch code, nn.Conv2d(in_channels=1, out_channels=32, ...) means the layer takes a 1-channel (grayscale) image as input and extracts 32 different types of features, producing a feature map with 32 channels.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**3. Padding: Preserving Image Size**<br>
The convolution operation naturally causes the output feature map to be smaller than the input image. This can lead to the loss of information at the edges and becomes problematic when building deep networks, as the feature map can become too small.

- **The Role of Padding**: To solve this, a border of values (usually zeros) is added around the input image to artificially increase its size. This process is called **padding**. By using appropriate padding, you can **maintain the same height and width for the input and output** of a convolution operation.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**4. Pooling: Feature Compression and Generalization**<br>
**Pooling** is a **downsampling** process that reduces the size of the feature map extracted by the convolution layer.

- **How it Works**: Similar to a kernel, it slides over a region and compresses the values in that region into a single representative value.
  - **Max Pooling**: Selects the **largest value** from the region. This has the effect of emphasizing the most prominent features detected in that area.

- **The Purpose of Pooling**:
  - **Computational Efficiency**: It reduces the size of the feature map, which decreases the number of parameters and the computational load for subsequent layers.
  - **Translation Invariance**: It makes the model more robust by helping it recognize a feature even if its position changes slightly in the image.
  - **Enlarging the Receptive Field**: It allows subsequent layers to see a summary of a wider area of the original input.
{% include end-box.html %}

## 4.Why Use CNNs?: Efficiency and Inductive Bias
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**1. Overwhelming Efficiency: A Reduction in the Number of Parameters**<br>
An MLP ignores the 2D structure of an image, flattening all pixels into a one-dimensional vector. This means that every single input pixel must be connected to every neuron in the next layer, which causes the required number of parameters (weights) to grow exponentially as the image size increases.

In contrast, a CNN utilizes the core idea of weight sharing.

- **For an MLP**: 68 parameters are needed to process a 4x4 image into 4 outputs. 

- **For a CNN**: The same task can be accomplished with a single 3x3 kernel that has only 10 parameters. 

Because this small kernel (filter) is reused and **shared** across the entire image, it can learn to extract features effectively with far fewer parameters. The MNIST example from the lecture material confirms this, showing that the simpler CNN model uses fewer parameters than the more complex MLP model (CNN: 421,642 vs. MLP: 535,818).
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**2. Smart Priors: Inductive Bias**<br>
**Inductive Bias** refers to the assumptions or constraints that a model has built-in to help it generalize beyond the training data. CNNs have two powerful inductive biases that are extremely well-suited for the characteristics of image data.

- **Locality**:<br> 
This is the assumption that a pixel is more highly correlated with its **immediate neighbors** than with pixels that are far away. CNNs naturally leverage this by using small kernels that only look at local regions of an image.

- **Translation Invariance and Weight Sharing**:<br> 
This is the assumption that a feature that is useful in one part of the image (e.g., a cat's eye) will be equally useful in another part. A CNN satisfies this assumption by applying the same kernel across the entire image (weight sharing). This allows the same "eye detector" kernel to find a cat's eye whether it's on the left or right side of the image.

Thanks to this inductive bias, a CNN can learn the structure of image data much more efficiently. This acts as a form of regularization, improving the model's ability to generalize. The graph on page 59 clearly demonstrates this: the CNN, with far fewer parameters, shows a smaller gap between its training and test error and ultimately achieves a lower test error than the fully connected network (MLP). 

**The Proof in the Experiments**<br>
The experimental results on pages 54 and 57 of the lecture material clearly show the effectiveness of CNNs.

- **MNIST Dataset**:<br> 
On simple digit images, both the MLP and CNN perform well, but the CNN achieves a slightly higher accuracy with fewer parameters (CNN 98.16% vs. MLP 96.19%).

- **Tiny ImageNet Dataset**:<br> 
The difference is stark on more complex, real-world images. The MLP essentially fails to learn (7.05% test accuracy), whereas the CNN performs significantly better (21.74%). Notably, the ResNet18 model pre-trained on ImageNet achieves overwhelming performance (51.51%), proving the importance of the CNN architecture and pre-training for complex image problems.
{% include end-box.html %}

## 5.CNN's Limitations and Evolution
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**The Limitation of Basic CNNs: The Limited Receptive Field**<br>

{% include end-box.html %}

{% include end-box.html %}















<div class="indented-paragraph" markdown="1">

</div>

{% include start-box.html class="math-box-inner" font_size="0.8em"%}

{% include end-box.html %}


{% include start-box.html class="math-box"%}

{% include end-box.html %}