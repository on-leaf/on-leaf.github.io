---
layout: single
title:  "1.Linear Algebra (2)"
category: "Mathematics for Machine Learning"
tag: [Machine Learning, Artificial Intelligence, Math]
header:
  teaser: #../images/2025-07-08-CT_6/image-20250708184503610.png
toc: true
toc_sticky: true
toc_label: CONTENTS
toc_icon: "fa-solid fa-seedling" # More icons: https://fontawesome.com/v6/search?ic=free
author_profile: false
sidebar:
    nav: "counts"
search: true # Change true to false if you don't want to have this article be searched 
redirect_from:
    - /Machine Learning/MfML_2
use_math: true
---

**[Reference]** <br>
$\bullet$ [MATHEMATICS FOR MACHINE LEARNING](https://mml-book.github.io/)
{: .notice--success}

# Introduction 
{% include start-box.html class="math-box"%}
Linear algebra is the study of vectors and certain rules to manipulate vectors.
Here, an arrow ($\rightarrow$) over letter represents a vector. 
{% include end-box.html %}

# 4. Vector Spaces
{% include start-box.html class="math-box"%}
In the following, we will have a closer look at vector spaces, i.e., a structured space in which vectors live.
We will start by introducing the concept of a group, which is a set of elements and an operation defined on these elements that keeps some structure of the set intact.

## 4-1) Groups
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 2.7 (Matrix)**<br>
Consider a set $\mathcal{G}$ and an operation $\otimes: \mathcal{G} \times \mathcal{G} \rightarrow \mathcal{G}$ defined on $\mathcal{G}$. 
Then $G:=(\mathcal{G}, \otimes)$ is called a group if the following hold:

1. _Closure_ of $\mathcal{G}$ under $\otimes$: $\forall x,y \in \mathcal{G}: x \otimes y \in \mathcal{G}$
2. Associativity: $\forall x, y, z \in G : (x \otimes y) \otimes z = x \otimes (y \otimes z)$
3. Neutral element: $\exists e \in G \, \forall x \in G : x \otimes e = x \text{ and } e \otimes x = x$
4. Inverse element: $\forall x \in G \, \exists y \in G : x \otimes y = e \text{ and } y \otimes x = e$, where $e$ is the neutral element. We often write $x^{-1}$ to denote the inverse element of $x$.

If additionally $\forall x,y \in \mathcal{G}: x \otimes y = y \otimes x$, then G = $(\mathcal{G}, \otimes)$ is an **_Abelian group_** (commutative).

x and y can be vectors or scalars. 
<div class="indented-paragraph" markdown="1">
These are letters representing elements of $\mathcal{G}$
</div>
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Example**<br>
Let us have a look at some examples of sets with associated operations and see whether they are groups:
* $(\mathbb{Z}, +)$ is an Abelian group. 
* $(\mathbb{N} \cup {0}, +)$ is not a group. 
    <div class="indented-paragraph" markdown="1">
    : Although $(\mathbb{N}_0, +)$ possesses a neutral element (0), the inverse elements are missing.
    </div>
* $(\mathbb{Z}, \cdot)$ is not a group.
    <div class="indented-paragraph" markdown="1">
    : Although $(\mathbb{Z}, \cdot)$ contains a neutral element (1), the inverse elements for any $z \in \mathbb{Z}, z \neq \pm 1$, are missing.
    </div>
* $(\mathbb{R}, \cdot)$ is not a group since 0 does not possess an inverse element.
* $(\mathbb{R} \setminus \{0\}, \cdot)$ is Abelian.
* $(\mathbb{R}^n, +), (\mathbb{Z}^n, +), n \in \mathbb{N}$ are Abelian if $+$ is defined componentwise, i.e., $(x_1, \dots, x_n) + (y_1, \dots, y_n) = (x_1 + y_1, \dots, x_n + y_n). \quad (4.1)$ <br>Then, $(x_1, \dots, x_n)^{-1} := (-x_1, \dots, -x_n)$ is the inverse element and $e = (0, \dots, 0)$ is the neutral element.
* $(\mathbb{R}^{m \times n}, +)$, the set of $m \times n$-matrices is Abelian (with componentwise addition as defined in (4.1)).
* $(\mathbb{R}^{n \times n}, \cdot)$
    - Closure and associativity follow directly 
    - Neutral element: **_I_**$_n$
    - The inverse **_A_**$^{-1}$ may exist or not. So, generally, it is not a group. However, the set of invertible matrices in $\mathbb{R}^{n \times n}$ with matrix multiplicationis a group, called **_general linear group_**. 
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**General Linear Group**<br>
The set of regular (invertible) matrices **_A_** $\in \mathbb{R}^{n \times n}$ is a group with respect to matrix multiplcation as defined in (2.3, $c_{ij} = \sum_{l=1}^{n} a_{il} b_{lj}, \quad i = 1, \ldots, m, \quad j = 1, \ldots, k$) and is called **_general linear group GL_** $(n,\mathbb{R})$. However, since matrix multiplication is not commutative, the group in not Abelian. 
{% include end-box.html %}

## 4-2) Vector Spaces
When we discussed groups, we looked at sets $\mathcal{G}$ and inner opeerations on $\mathcal{G}$, i.e., mappings $\mathcal{G} \times \mathcal{G} \rightarrow \mathcal{G}$ that only operate on elements in $\mathcal{G}$. 
We will consider sets with an inner operation $+$, an outer operation $\cdot$ and the multiplication of a vector $x \in \mathcal{G}$ by a scalar $\lambda \in \mathbb{R}$. 
<div class="indented-paragraph" markdown="1">
We can think of the inner operation as a form of addition, and the outer operation asa form of scailing, but the inner/outer operations have nothing to do with inner/outer products.
</div>

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 2.9 (Vector Space)**<br>
A real-valued _vector space_ $V = (\mathcal{V}, +, \cdot)$ is a set $\mathcal{V}$ with two operations

<center>$$ +: \mathcal{V} \times \mathcal{V} \rightarrow \mathcal{V} \tag{4.2}$$</center>
<center>$$ \cdot: \mathcal{R}\times \mathcal{V} \rightarrow \mathcal{V} \tag{4.3}$$</center>

where

1. $(\mathcal{V}, +)$ is an Abelian group (Five properties)
2. Distributivity:<br>
    1) $\forall \lambda \in \mathbb{R}, \vec{x}, \vec{y} \in \mathcal{V} : \lambda \cdot (\vec{x}+\vec{y}) = \lambda \cdot \vec{x} + \lambda \cdot \vec{y}$<br>
    2) $\forall \lambda, \psi \in \mathbb{R}, \vec{x} \in \mathcal{V} : (\lambda + \psi) \cdot \vec{x} = \lambda \cdot \vec{x} + \psi \cdot \vec{x}$
3. Associativity (outer operation): $\forall \lambda,\psi \in \mathbb{R}, \vec{x} \in \mathcal{V}: \lambda \cdot(\psi \cdot \vec{x}) = (\lambda \psi)\cdot \vec{x}$
4. Neutral element with repsect to the outer operation: $\forall \vec{x} \in \mathcal{V}: 1 \cdot \vec{x} = \vec{x}$

    <div class="indented-paragraph" markdown="1">

    - The elements $\vec{x} \in V$ are called **_vectors_**.
    - The neutral element of $(\mathcal{V}, +)$ is the zero vector $\vec{0}$ = $[0, \dots, 0]^{\top}$
    - The inner operation $+$ is called _vector addition_
    - The elements $\lambda \in \mathbb{R}$ are called _scalars_ and the outer operation $\cdot$ is a _multiplication by scalars_.
    </div>

In addition to five properties of Abelian group, distributivity, associativity and neutral element conditions, total eight properties, are conditions for vector space. 
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Notation of vector**<br>
We will write $n$-tuples as **_column vectors_** 

$$\vec{x}= \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} \in \mathbb{R}^{n} = \mathbb{R}^{n \times 1} \tag{4.4}$$

{% include end-box.html %}


## 4-3) Vector Subspaces
Vector subspaces are sets contained in the original vector space with the property that when we perform vector space operations on elements within this subspace, we will never leave it. 
<div class="indented-paragraph" markdown="1">
Closed!
</div>
Vector subspaces are a key idea in machine learning. 
<div class="indented-paragraph" markdown="1">
It used to dimensionality reduction
</div>

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 2.10 (Vector Subspace)**<br>
Let _V_ = $(\mathcal{V}, +, \cdot)$ be a vector space and $\mathcal{U} \subseteq \mathcal{V}, \mathcal{U} \neq \emptyset$.
Then _U_ = $(\mathcal{U}, +, \cdot)$ is called **_vector subspace_** of _V_ (or **_linear subspace_**) if _U_ is a vector space with the vector space operations $+$ and $\cdot$ restricted to $\mathcal{U} \times \mathcal{U}$ and $\mathcal{R} \times \mathcal{U}$.

<div class="indented-paragraph" markdown="1">
_U_ $\subseteq$ _V_: _U_ is subspace of _V_
</div>

If $\mathcal{U} \subseteq \mathcal{V}$ is a vector space, _U_ naturally inherits many properties directly from _V_. (Since $\vec{x} \in \mathcal{U} \subseteq \mathcal{V}$) 
<div class="indented-paragraph" markdown="1">

- The Abelian group properties
- The distributivity
- The associativity
- The neutral element
</div>

To determine whether $(\mathcal{U}, +, \cdot)$ is a subspace of _V_ we need to show
1. $\mathcal{U} \neq \emptyset$, in particular: $\vec{0} \in \mathcal{U}$
2. Closure of _U_:<br>
    1) About outer operation: $\forall \lambda \in \mathbb{R} \, \forall x \in \mathcal{U} : \lambda \vec{x} \in \mathcal{U}$<br>
    2) About inner operation: $\forall \vec{x}, \vec{y} \in \mathcal{U} : \vec{x} + \vec{y} \in \mathcal{U}$
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Example**<br>
- For every vector space _V_, _V_ and {$\vec{0}$} are the trivial subspaces.
- The solution set of **_A_**$\vec{x} = \vec{0}$ with $n$ unknowns $$\vec{x}$$ = $[x_1, \dots, x_n]^{\top}$ is the subspace of $\mathbb{R}^n$.
- The solution of **_A_**$\vec{x}$ = $\vec{b}$ ($\vec{b} \neq \vec{0}$) is not a subspace of $\mathbb{R}^{n}$.
- The interection of arbitrarily many subspaces is subspace itself.
{% include end-box.html %}
{% include end-box.html %}




# 5. Linear Independence
{% include start-box.html class="math-box"%}
Here, we will look at what we can do with elements of the vector space. 
The closure property guarantees that vector addition and vector multiplication end up with another vector in the same vector space.
Then, it is possible to find a set of vectors with which we can represent every vector in the vector space by adding and scailing them together. 
<div class="indented-paragraph" markdown="1">
This set of vectors is a **_basis_**
</div>
Before introduce basis, we need to know about concepts of linear combinations and linear independence.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 2.11 (Linear Combination)**<br>
Consider a vector space _V_ and a finite number of vectors $x_1, \dots, x_k \in V$.
Then every $v \in V$ of the form 

$$\vec{v} = \lambda_1 \vec{x}_1 + \dots + \lambda_k \vec{x}_k  = \sum_{i=1}^{k} \lambda_i \vec{x}_i \in V \tag{5.1}$$

with $\lambda_1, \dots, \lambda_k \in \mathbb{R}$ is **_linear combination_** of the vectors $\vec{x}_1, \dots, \vec{x}_k$.
{% include end-box.html %}

We are  interested in non-trivial linear combinations of a set of vectors to represent $\vec{0}$, i.e., linear combinations of vectors $\vec{x}_1, \dots, \vec{x}_k$, where not all coefficients $\lambda_i$ in (5.1) are 0.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 2.12 (Linear (In)dependence))**<br>
Consider a vector space _V_ with $k \in \mathbb{N}$ and $\vec{x}_1, \dots, \vec{x}_k \in V$.

If there is a non-trivial linear combination, such that $\vec{0} = \sum_{i=1}^{k} \lambda_i \vec{x}_i$ with at least one $\lambda_i \neq 0$, the vectors $\vec{x}_1, \dots, \vec{x}_k$ are **_linearly dependent_**.

If only the trivial solution exists, i.e., $\lambda_1 = \dots = \lambda_k = 0$ the vectors $\vec{x}_1, \dots, \vec{x}_k$ are **_linearly independent_**. 
{% include end-box.html %}

Then, how can we find out whether vectors are linearly independent or not? 
A practical way of checking linearity of vectors is to use Gaussian elimination. 
<div class="indented-paragraph" markdown="1">
Write all vectors as columns of a matrix **_A_** and perform Gaussian elimination until the matrix is in row echelon form
</div>

- The pivot columns indicate the vectors, which are linearly independent of the vectors on the left. 
- The non-pivot columns can be expressed as linear combinations of the pivot columns on their left. 
    <div class="indented-paragraph" markdown="1">
    All column vectors are linearly independent if and only if all columns are pivot columns. 
    </div>

For example, 
<center>$$\begin{bmatrix} 1 & 1 & -1 \\ 2 & 1 & -2 \\ -3 & 0 & 1 \\ 4 & 2 & 1 \end{bmatrix} \text{~} \begin{bmatrix} 1 & 1 & -1 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{bmatrix}$$</center>
Here, every column of the matrix is a pivot column. 
Thus, vectors, $[1 \ 2 \ -3 \ 4]^{\top}, [1 \ 1 \ 0 \ 2]^{\top}, [-1 \ -2 \ 1 \ 1]^{\top}$, are linearly independent. 

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
Consider a vector space _V_ with k linearly independent vectors $b_1, \dots, b_k$ and $m$ linear combinations 

$$\begin{gather*}
\vec{x}_1 = \sum_{i=1}^{k} \lambda_{i1} \vec{b}_i, \\
\vdots \\
\vec{x}_m = \sum_{i=1}^{k} \lambda_{im} \vec{b}_i.
\end{gather*} \tag{5.2}$$

Defining **_B_** = $[\vec{b}_1, \dots, \vec{b}_k]$ as the matrix whose columns are the linearly independent vectors $\vec{b}_1, \dots, \vec{b}_k$, we can write

$$\vec{x}_j = B\lambda_j, \vec{\lambda}_j = \begin{bmatrix} \lambda_{1j} \\ \vdots \\ \lambda_{kj} \end{bmatrix}, j=1,\dots, m, \tag{5.3}$$

in a more compact form. 

To test wheter $$\vec{x}_1, \dots, \vec{x}_m$$ are linearly independent, we follow the general approach of testing when $\sum_{j=1}^{m} \psi_j \vec{x}_j = \vec{0}$.
With (5.2), we obtain

$$\sum_{j=1}^{m} \psi_j x_j = \sum_{j=1}^{m} \psi_j B \lambda_j = B \sum_{j=1}^{m} \psi_j \lambda_j. \tag{5.4}$$

This means that ${\vec{x}_1, \dots, \vec{x}_m}$ are linearly independent if and only if the column vectors ${\vec{\lambda}_1, \dots, \vec{\lambda}_m}$ are linearly independent.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Example**<br>
Consider a set of linearly independent vectors $\vec{b}_1, \vec{b}_2, \vec{b}_3, \vec{b}_4 \in \mathbb{R}^{n}$ and

$$\begin{aligned}
    x_1 &= b_1 &- 2b_2 &+ b_3 &- b_4 \\
    x_2 &= -4b_1 &- 2b_2 & &+ 4b_4 \\
    x_3 &= 2b_1 &+ 3b_2 &- b_3 &- 3b_4 \\
    x_4 &= 17b_1 &- 10b_2 &+ 11b_3 &+ b_4
\end{aligned} \tag{5.5}$$

To check the independency of vectors $\vec{x}_1, \dots, \vec{x}_4 \in \mathbb{R}^{n}$, we investigate whether the column vectors

$$\left\{
\begin{bmatrix} 1 \\ -2 \\ 1 \\ -1 \end{bmatrix},
\begin{bmatrix} -4 \\ -2 \\ 0 \\ 4 \end{bmatrix},
\begin{bmatrix} 2 \\ 3 \\ -1 \\ -3 \end{bmatrix},
\begin{bmatrix} 17 \\ -10 \\ 11 \\ 1 \end{bmatrix}
\right\} \tag{5.6}$$

are linearly independent. 

The reduced row-echelon form of the corresponding linear equation system with coefficient matrix 

$$A = \begin{bmatrix}
    1 & -4 & 2 & 17 \\
    -2 & -2 & 3 & -10 \\
    1 & 0 & -1 & 11 \\
    -1 & 4 & -3 & 1
\end{bmatrix} \tag{5.7}$$

is given as

$$\begin{bmatrix}
    1 & 0 & 0 & -7 \\
    0 & 1 & 0 & -15 \\
    0 & 0 & 1 & -18 \\
    0 & 0 & 0 & 0
\end{bmatrix}. \tag{5.8}$$

Since there are no 4-th pivot column, vectors $\vec{x}_1, \dots, \vec{x}_4$, are linearly dependent. 
{% include end-box.html %}
{% include end-box.html %}

# 6. Basis and Rank
{% include start-box.html class="math-box"%}
In a vector space _V_, we are particularly interested in sets of vectors $\mathcal{A}$ that possess the property that any vector $\vec{v} \in V$ can be obtained by a linear combination of vectors in $\mathcal{A}$. 
These vectors are special vectors, and in the following, we will characterize them.

## 6-1) Generating Set and Basis
{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 2.13 (Generating Set and Span)**<br>
Consider a vector space $V = (\mathcal{V}, +, \cdot)$ and set of vectors $$\mathcal{A} =  \{ \vec{x}_1, \cdots, \vec{x}_{k} \} \subseteq \mathcal{V}$$.
If every vector $\vec{v} \in \mathcal{V}$ can be expressed as a linear combination of $$\vec{x}_1, \dots, \vec{x}_k, \mathcal{A}$$ is called a **_generating set_** of _V_.

<div class="indented-paragraph" markdown="1">
The set of all linear combinations of vectors in $\mathcal{A}$ is called the **_span_** of $\mathcal{A}$.

_V_ = span[$\mathcal{A}$] or _V_ = span[$\vec{x}_1, \dots, \vec{x}_k$]
</div>
{% include end-box.html %}

Every vector can be represented as a linear combination of the vectors in the generating set. 
Now, let's characterize the smallest generating set that spans a vector (sub)space. 

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition 2.14 (Basis)**<br>
Consider a vector space $V = (\mathcal{V}, +, \cdot)$ and $\mathcal{A} \subseteq \mathcal{A}$. 
A generating set $\mathcal{A}$ of _V_ is called _minimal_ if there exists no smaller set $\tilde{\mathcal{A}} \subsetneq \mathcal{A} \subseteq \mathcal{V}$ that spans $V$.
<div class="indented-paragraph" markdown="1">
Every linearly independent generating set of $V$ is minimal and is called a **_basis_** of $V$.
</div>

Every vector space $V$ possesses a basis $\mathcal{B}$.
There can be many bases of a vector space $V$.
<div class="indented-paragraph" markdown="1">
No unique basis.
</div>
However, all bases possess the same number of elements.
{% include end-box.html %}

We only consider finite-dimensional vector spaces _V_.
<div class="indented-paragraph" markdown="1">
The **_dimension_** of $V$ is the number of basis vectors of $V$, and we write dim($V$)
</div>
If $U \subseteq V$ is a subspace of $V$, then dim($U$) $\le$ dim($V$).<br>
If $U = V$, dim($U$) = dim($V$).

<div class="indented-paragraph" markdown="1">
The dimension of a vector space is the number of independent directions in this vector space
</div>


{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Example (Determining a Basis)**<br>
A basis of a subspace $U = \text{span}[\vec{x}_1, \dots, \vec{x}_m] \subseteq \mathbb{R}^{n}$ can be found be executing the following steps:
1. Write the spanning vectors as columns of a matrix $A$
2. Determining the row-echelon form of $A$
3. The spanning vectors associated with the pivot columns are a basis of $U$

For a vector subspace $U \subseteq \mathbb{R}^5$, spanned by the vectors 

$$x_1 = \begin{bmatrix} 1 \\ 2 \\ -1 \\ -1 \\ -1 \end{bmatrix}, \quad x_2 = \begin{bmatrix} 2 \\ -1 \\ 1 \\ 2 \\ -2 \end{bmatrix}, \quad x_3 = \begin{bmatrix} 3 \\ -4 \\ 3 \\ 5 \\ -3 \end{bmatrix}, \quad x_4 = \begin{bmatrix} -1 \\ 8 \\ -5 \\ -6 \\ 1 \end{bmatrix} \in \mathbb{R}^5. \tag{5.9}$$

We can write matrix $A$ as

$$\begin{bmatrix} x_1, & x_2, & x_3, & x_4 \end{bmatrix} = 
\begin{bmatrix} 1 & 2 & 3 & -1 \\2 & -1 & -4 & 8 \\-1 & 1 & 3 & -5 \\-1 & 2 & 5 & -6 \\-1 & -2 & -3 & 1 \end{bmatrix} \text{~} \begin{bmatrix}1 & 2 & 3 & -1 \\0 & 1 & 2 & -2 \\0 & 0 & 0 & 1 \\0 & 0 & 0 & 0 \\0 & 0 & 0 & 0\end{bmatrix} \tag{5.10}$$

Since the pivot columns indicate which set of vectors is linearly independent, we can see that $\{\vec{x}_1, \vec{x}_2, \vec{x}_4 \}$ are linearly independent.
Therefore, $\{\vec{x}_1, \vec{x}_2, \vec{x}_4 \}$ is a basis of $U$.
{% include end-box.html %}

## 6-2) Rank
The number of linearly independent columns of a matrix $A \in \mathbb{R}^{m \times n}$

{% include end-box.html %}


