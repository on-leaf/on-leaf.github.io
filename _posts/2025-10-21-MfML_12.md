---
layout: single
title:  "6.Probability and Distributions"
category: "Mathematics for Machine Learning"
tag: [Machine Learning, Artificial Intelligence, Math]
header:
  teaser: #../images/2025-07-08-CT_6/image-20250708184503610.png
toc: true
toc_sticky: true
toc_label: CONTENTS
toc_icon: "fa-solid fa-seedling" # More icons: https://fontawesome.com/v6/search?ic=free
author_profile: false
sidebar:
    nav: "counts"
search: true # Change true to false if you don't want to have this article be searched 
redirect_from:
    - /Machine Learning/MfML_12
use_math: true
---

**[Reference]** <br>
$\bullet$ [MATHEMATICS FOR MACHINE LEARNING](https://mml-book.github.io/)
{: .notice--success}

# Introduction 
{% include start-box.html class="math-box"%}
Probability is the study of uncertainty. 
Probability distributions are used as a building block for other concepts, such as probabilistic modeling, graphical models, and model selection.
In this section, we will look at:
- A probability space (the sample space, the events, and the probability of an event)
- The random variable
{% include end-box.html %}

# 1.Construction of a Probability Space
{% include start-box.html class="math-box"%}
The theory of probability aims at defining a mathematical structure to describe random outcomes of experiments.
Using this mathematical structure of probability, the goal is to perform automated reasoning, and in this sense, probability generalizes logical reasoning.

## 1-1) Philosophical Issues
Everyday reasoning and machine learning problems involve **uncertainty**, which is hard to express with simple true/false logic. 
**Probability theory** provides a mathematical framework to handle this **plausibility**, extending logic to enable **automated reasoning** under uncertainty.

In machine learning and statistics, there are two major ways to interpret probability:
- **Bayesian**: Probability represents a **degree of belief** or subjective certainty.
- **Frequentist**: Probability is the **relative frequency** of an event occurring over many trials in the long run.

Be aware that terms like "probability distribution" can sometimes be used ambiguously in machine learning literature. It's often helpful to clarify whether the context involves modeling **categorical** (discrete) or **continuous** variables.

## 1-2) Probability and Random Variables
To handle probability mathematically, we need to know the foundational framework first:
- The probability space
- A random variable

{% include start-box.html class="math-box-inner" font_size="0.9em"%}
**Probability Space**<br>
A probability space is composed of three elements to mathematically model the random outcomes of an experiment.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Sample space ($\Omega$)**<br>
The set of all possible elementary outcomes of the experiment.

Ex) (Two coin tosses): $$\Omega = \{hh, ht, th, tt\}$$
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Evnet space ($\mathcal{A}$)**<br>
A collection of events we are interested in, where each event is a subset of the sample space $\Omega$. (Technically, it must satisfy certain mathematical conditions.)

Ex) "Event of getting exactly one head" = $$\{ht, th\}$$
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Probability measure ($P$)**<br>
A function that assigns a probability $P(A)$ (a real number between 0 and 1) to each event $A \in \mathcal{A}$, representing the likelihood of that event occurring.

Ex) $$P(\{ ht, th \}) = 1/2$$
{% include end-box.html %}

Modern probability theory is based on this probability space $(\Omega, \mathcal{A}, P)$.
{% include end-box.html %}


{% include start-box.html class="math-box-inner" font_size="0.9em"%}
**Random variable**<br>
Random variable translates outcomes to numbers $X: \Omega \to \mathcal{T}$

In practice, rather than working directly with the potentially complex sample space $\Omega$, we often use a function that maps each outcome to a more convenient value (usually a number). This function is called a Random Variable $X$.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Target Space ($\mathcal{T}$)**<br>
The set of all possible values that the random variable $X$ can take (its range). 
The type of this space determines whether the random variable is discrete or continuous.
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Random Variable ($X$)**<br>
A function that maps each element $\omega$ in the sample space $\Omega$ to a specific value $X(\omega)$ in the target space $\mathcal{T}$.

Ex) Example (Counting heads in two coin tosses):
- $X(hh) = 2$
- $X(ht) = 1$
- $X(th) = 1$
- $X(tt) = 0$
- In this case, the target space is $\mathcal{T} = \{0, 1, 2\}$.

Despite its name, a random variable is neither random nor a variable; it is a **function**.
{% include end-box.html %}
{% include end-box.html %}

What we are often interested in is the probability within the target space $\mathcal{T}$. For instance, what's the probability of getting exactly one head ($P(X=1)$)?

This is linked back to the original probability space via the random variable $X$. 
The probability $P_X(S)$ of an event $S \subseteq \mathcal{T}$ in the target space is equal to the probability $P$ of the set of elements in the original sample space $\Omega$ that map to $S$ under $X$ (this set is called the pre-image $X^{-1}(S)$).

$$P_X(S) := P(X \in S) = P(X^{-1}(S)) = P(\{\omega \in \Omega \mid X(\omega) \in S\})$$

Ex) $$P_X(X=1) = P(\{ \omega \in \Omega \mid X(\omega) = 1 \}) = P(\{ ht, th \}) = 1/2$$

This function $P_X$ is called the **probability distribution** or **law** of the random variable $X$.

## 1-3) Statistics
In the following, we will look at the differences between **Probability Theory** and **Statistics**, and how they relate to Machine Learning. 
Both deal with uncertainty, but they approach it from opposite directions.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Probability Theory: Cause $\to$ Effect (Model → Data)**<br>
- Perspective: Given a probabilistic model (e.g., a fair coin), it predicts what outcomes (data) are likely to result from it.

- Process: Assumes an underlying process (model) with uncertainty and uses the rules of probability to deduce what might happen in the future.

- Example: "If I flip a fair coin 100 times, about how many heads should I expect?"
{% include end-box.html %}

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Statistics: Effect $\to$ Cause (Data → Model)**<br>
- Perspective: Given observed results (data), it infers what underlying process (model) might have generated that data.

- Process: Starts with the observed data and tries to find the probabilistic model that best explains it.

- Example: "I flipped a coin 100 times and got 70 heads. Can I conclude the coin is fair? If not, what is the estimated probability of getting heads?"
{% include end-box.html %}

The goal of machine learning is to build a **model** that best explains and **generalizes** from given **data**. 
This is very similar to the goal of **statistics**, which infers the underlying process from observations. 
Machine learning uses the rules of probability to find the "best-fitting" model for the data.

Machine learning aims for models that perform well not just on the training data, but also on **new**, **unseen future data**. 
Analyzing and predicting this future performance (generalization ability) relies heavily on both probability and statistics.
{% include end-box.html %}











<div class="indented-paragraph" markdown="1">

</div>

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition . ()**<br>
{% include end-box.html %}


{% include start-box.html class="math-box"%}

{% include end-box.html %}