---
layout: single
title:  "5.Vector Calculus (2)"
category: "Mathematics for Machine Learning"
tag: [Machine Learning, Artificial Intelligence, Math]
header:
  teaser: #../images/2025-07-08-CT_6/image-20250708184503610.png
toc: true
toc_sticky: true
toc_label: CONTENTS
toc_icon: "fa-solid fa-seedling" # More icons: https://fontawesome.com/v6/search?ic=free
author_profile: false
sidebar:
    nav: "counts"
search: true # Change true to false if you don't want to have this article be searched 
redirect_from:
    - /Machine Learning/MfML_10
use_math: true
---

**[Reference]** <br>
$\bullet$ [MATHEMATICS FOR MACHINE LEARNING](https://mml-book.github.io/)
{: .notice--success}

# Introduction 
{% include start-box.html class="math-box"%}
Many algorithms in machine learning can be framed as optimization problems, where we seek to find the best model parameters that describe a set of data. 
To solve these problems, we need a systematic way to find the direction of steepest ascent, which is given by the gradient.

This chapter introduces the fundamental tools of vector calculus needed to compute these gradients. 
We assume that funcitons are differentiable.
{% include end-box.html %}

# 6.Backpropagation and Automatic Differentiation
{% include start-box.html class="math-box"%}
In many machine learning applications, we find good model parameters by performing gradient descent. For a given objective function, we can obtain the gradient with respect to the model parameters using calculus and applying the chain rule. However, for deep learning models where numerous functions are nested, deriving the differentiation formula by hand leads to many problems - _Too slow and complex_.

**_Backpropagation_** is a brilliant algorithm that solves these problems. 
Instead of deriving the entire differentiation formula at once, it breaks down the computation into steps and efficiently calculates the derivative at each point by tracing backward.
In conclusion, backpropagation is the core engine that quickly and automatically computes the gradients of complex deep learning models, and it can be credited as a key contributor that made today's deep learning revolution possible.

## 6-1) Gradients in a Deep Newwork
One of the core components of deep learning model is backpropagation algorithm. A deep learning model is essentially a giant **composite function**, where numerous functions $(f_1, f_2,\dots, )$ are layered on top of each other. Input data ($x$) passes through these functions in sequence to produce a final prediction ($y$), and the difference between this prediction and the actual value is calculated as the loss ($L$).

Our goal is to find the gradient of this final loss ($L$) with respect to all the parameters ($A_i, b_i$) that make up the model, in order to minimize the loss. The gradient of such a deeply nested function can be found using the chain rule from mathematics. To find the gradient of the loss ($L$) with respect to a specific parameter ($\theta_i$), we must apply the chain rule continuously:

$$\frac{\partial L}{\partial \theta_i} = \frac{\partial L}{\partial f_K} \frac{\partial f_K}{\partial f_{K-1}} \cdots \frac{\partial f_{i+1}}{\partial \theta_i} \tag{5.34}$$

An important pattern emerges here: to calculate the gradient of any layer, you always need the gradient calculated from the next layer ($\frac{\partial L}{\partial f_{i+1}}$).

Backpropagation is an algorithm that leverages this very pattern. As its name suggests, it starts from the final loss $L$ and propagates the gradient at each point by moving backward through the neural network. The core of backpropagation lies in the reuse of calculations. The gradient value calculated at one layer is directly reused to calculate the gradient of the previous layer. This dramatically reduces redundant calculations, leading to immense computational efficiency.

In conclusion, backpropagation is the core engine that calculates the gradients of the giant composite function of deep learning in a very fast and efficient manner, based on the chain rule.

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Example (Backpropagation)**<br>

Consider the function and break down the complex function $f(x)$ into a sequence of simple steps as below.

<figure style="display: flex; flex-direction: column; align-items: center; margin-top: 0.5em; margin-bottom: 0.5em;">
  <img src="../images/2025-10-17-MfML_11/Fig_1.png" alt="" 
       style="width: 70%; height: auto;">
   <figcaption style="font-size: 20px; margin-top: -0.5em;">
   </figcaption>
</figure> 

$$f(x) = \sqrt{x^2 + \exp(x^2)} + \cos(x^2 + \exp(x^2)), \quad \begin{cases}
a &= x^2, \\
b &= \exp(a), \\
c &= a + b, \\
d &= \sqrt{c}, \\
e &= \cos(c), \\
f &= d + e.
\end{cases}$$

This process of calculating from the input $x$ to the final output $f$ is called the forward pass.
Since each step is a very simple function, we can easily compute its derivative with respect to its input (the local gradient).

$$\begin{align*}
\frac{\partial a}{\partial x} &= 2x \\
\frac{\partial b}{\partial a} &= \exp(a) \\
\frac{\partial c}{\partial a} &= 1 = \frac{\partial c}{\partial b} \\
\frac{\partial d}{\partial c} &= \frac{1}{2\sqrt{c}} \\
\frac{\partial e}{\partial c} &= -\sin(c) \\
\frac{\partial f}{\partial d} &= 1 = \frac{\partial f}{\partial e}
\end{align*}$$

Now, to find our final goal, $\frac{\partial f}{\partial x}$, we apply the chain rule starting from the end of the graph and work our way backward to compute the derivatives.

**1. Start**: $\frac{\partial f}{\partial f} = 1$ (The derivative of a variable with respect to itself is 1).

**2. From f to d, e: Since $f=d+e$,**
- $\frac{\partial f}{\partial d} = \frac{\partial f}{\partial f} \frac{\partial f}{\partial d} = 1 \cdot 1 = 1$
- $\frac{\partial f}{\partial e} = \frac{\partial f}{\partial f} \frac{\partial f}{\partial e} = 1 \cdot 1 = 1$

**3. From d, e to c: Since $c$ affects both $d$ and $e$, we sum the gradients from both paths.**
- $\frac{\partial f}{\partial c} = \frac{\partial f}{\partial d}\frac{\partial d}{\partial c} + \frac{\partial f}{\partial e}\frac{\partial e}{\partial c} = (1) \cdot (\frac{1}{2\sqrt{c}}) + (1) \cdot (-\sin(c))$

**4. From c to a, b: We use the value of $\frac{\partial f}{\partial c}$ that we just computed.**
- $\frac{\partial f}{\partial b} = \frac{\partial f}{\partial c}\frac{\partial c}{\partial b} = \frac{\partial f}{\partial c} \cdot 1$
- $\frac{\partial f}{\partial a} = \frac{\partial f}{\partial b}\frac{\partial b}{\partial a} + \frac{\partial f}{\partial c}\frac{\partial c}{\partial a} = (\frac{\partial f}{\partial b}) \cdot (\exp(a)) + (\frac{\partial f}{\partial c}) \cdot 1$

**5. From a to x: Finally, we compute $\frac{\partial f}{\partial x}$.**
- $\frac{\partial f}{\partial x} = \frac{\partial f}{\partial a}\frac{\partial a}{\partial x} = (\frac{\partial f}{\partial a}) \cdot (2x)$

<br>

Here, we can know that 
- This example clearly shows that backpropagation is an algorithm that breaks down a complex function into a simple computation graph and uses the chain rule to compute gradients by working backward from the output to the input.
- We can see that the gradient value calculated in a later step is reused to compute gradients in earlier steps (e.g., $\frac{\partial f}{\partial c}$ is used to compute both $\frac{\partial f}{\partial a}$ and $\frac{\partial f}{\partial b}$). This is why backpropagation is so efficient.
- Algorithmic approach of backpropagation can compute the final derivative formula with a computational complexity similar to that of the function itself.

{% include end-box.html %}

{% include end-box.html %}
















<div class="indented-paragraph" markdown="1">

</div>

{% include start-box.html class="math-box-inner" font_size="0.8em"%}
**Definition . ()**<br>
{% include end-box.html %}


{% include start-box.html class="math-box"%}

{% include end-box.html %}