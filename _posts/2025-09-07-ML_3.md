---
layout: single
title:  "3.Measuring performance"
category: "Machine Learning"
tag: [Machine Learning, Artificial Intelligence]
header:
  teaser: #../images/2025-07-08-CT_6/image-20250708184503610.png
toc: true
toc_sticky: true
toc_label: CONTENTS
toc_icon: "fa-solid fa-seedling" # More icons: https://fontawesome.com/v6/search?ic=free
author_profile: false
sidebar:
    nav: "counts"
search: true # Change true to false if you don't want to have this article be searched 
redirect_from:
    - /Machine Learning/ML_2
use_math: true
---

**[Reference]** <br>
$\bullet$ [Understanding Deep Learning](https://udlbook.github.io/udlbook/)
{: .notice--success}

# Introduction
{% include start-box.html class="math-box"%}
This page considers how to measure the performance of the trained model. 
{% include end-box.html %}

# 1. Goal of training/learning
{% include start-box.html class="math-box"%}
The main goal of training/learning of model is minimizing error.
{% include end-box.html %}

# 2. Soucres of error
{% include start-box.html class="math-box"%}
There are three possible sources of error, which are known as noise, bias, and variance respectively.

<figure style="display: flex; flex-direction: column; align-items: center; margin-top: 0.5em; margin-bottom: 0.5em;">
  <img src="../images/2025-09-07-ML_3/Fig1.png" alt="Noise, Bias, Variance" 
       style="width: 90%; height: auto;">
   <figcaption style="font-size: 20px; margin-top: -0.5em;">
   Fig.2.1. Sources of test error. a) Noise. Data generation is noisy, so even if the model exactly replicates the true underlying function (black line), the noise in the
            test data (gray points) means that some error will remain (gray region represents two standard deviations). b) Bias. Even with the best possible parameters, the three-region model (cyan line) cannot exactly fit the true function (black line). This bias is another source of error (gray regions represent signed error). c) Variance. In practice, we have limited noisy training data (orange points). When we fit the model, we donâ€™t recover the best possible function from panel (b) but a slightly different function (cyan line) that reflects idiosyncrasies of the training data. This provides an additional source of error (gray region represents two standard deviations). 
   </figcaption>
</figure> 

## 2-1) Noise 
The data generation process includes the addition of noise. 
This source of error is insurmountable for the test data. (e.g. Physical sensor noise, Labeling human error, etc)

## 2-2) Bias
Model inflexibility (related to capacity) prevents perfectly fitting true function.
<div class="indented-paragraph" markdown="1">
Ex) The three-region neural network model cannot exactly describe the quasi-sinusoidal function
</div>
This is known as bias.

## 2-3) Variance
Limited data causes variability in learned model outcomes
<div class="indented-paragraph" markdown="1">
If using partial data, the estimation varies within some ranges
</div>
{% include end-box.html %}

# 3. Mathematical formulation of test error
{% include start-box.html class="math-box"%}
Consider a 1D regression problem where the data generation process has additive noise with variance $\sigma^2$ (Fig 3.1).

<figure style="display: flex; flex-direction: column; align-items: center; margin-top: 0.5em; margin-bottom: 0.5em;">
  <img src="../images/2025-09-07-ML_3/Fig2.png" alt="Noised data" 
       style="width: 40%; height: auto;">
   <figcaption style="font-size: 20px; margin-top: -0.5em;">
   Fig.3.1. Data and noise with variance $\sigma^2$.
   </figcaption>
</figure> 

For a given dataset, the error comes from 

$$\mathbb{E}_{\mathcal{D}}\big[\mathbb{E}_{y}[L[x]]\big] = \underbrace{\mathbb{E}_{\mathcal{D}}\Big[ \big(f[x, \phi[\mathcal{D}]] - f_{\mu}[x]\big)^{2} \Big]}_{\text{variance}} + \underbrace{\big(f_{\mu}[x] - \mu[x]\big)^{2}}_{\text{bias}} + \underbrace{\sigma^{2}}_{\text{noise}}\tag{3.1}$$

Thus, reducing error means reducing variance and bias.
{% include end-box.html %}